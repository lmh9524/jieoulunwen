#!/usr/bin/env python3
"""
æ•°æ®é›†å®Œæ•´æ€§æ£€æŸ¥è„šæœ¬
æ£€æŸ¥VAWå’ŒCelebAæ•°æ®é›†çš„å®Œæ•´æ€§å¹¶ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
"""

import os
import json
from pathlib import Path

def check_file_exists_and_size(filepath):
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¹¶è¿”å›å¤§å°ä¿¡æ¯"""
    if os.path.exists(filepath):
        size = os.path.getsize(filepath)
        return {
            'exists': True,
            'size': size,
            'size_mb': round(size / (1024*1024), 2),
            'status': 'âœ…' if size > 1000 else 'âš ï¸'
        }
    else:
        return {
            'exists': False,
            'size': 0,
            'size_mb': 0,
            'status': 'âŒ'
        }

def check_vaw_dataset():
    """æ£€æŸ¥VAWæ•°æ®é›†å®Œæ•´æ€§"""
    print("=== VAWæ•°æ®é›†å®Œæ•´æ€§æ£€æŸ¥ ===")
    
    base_dir = Path(r'D:\KKK\data\VAW')
    annotations_dir = base_dir / 'annotations'
    images_dir = base_dir / 'images'
    
    # æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶
    annotation_files = ['train_part1.json', 'train_part2.json', 'val.json', 'test.json']
    
    for filename in annotation_files:
        filepath = annotations_dir / filename
        info = check_file_exists_and_size(filepath)
        print(f"  {info['status']} {filename}: {info['size_mb']} MB")
        
        # éªŒè¯JSONæ ¼å¼
        if info['exists'] and info['size'] > 1000:
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        print(f"    ğŸ“Š åŒ…å« {len(data)} æ¡è®°å½•")
                    elif isinstance(data, dict):
                        print(f"    ğŸ“Š åŒ…å« {len(data)} ä¸ªé”®")
            except Exception as e:
                print(f"    âš ï¸ JSONæ ¼å¼é”™è¯¯: {e}")
    
    # æ£€æŸ¥å›¾åƒç›®å½•
    if images_dir.exists():
        image_count = len(list(images_dir.glob('*.jpg'))) + len(list(images_dir.glob('*.png')))
        print(f"  ğŸ“ å›¾åƒç›®å½•: {image_count} ä¸ªå›¾åƒæ–‡ä»¶")
    else:
        print("  âŒ å›¾åƒç›®å½•ä¸å­˜åœ¨")

def check_celeba_dataset():
    """æ£€æŸ¥CelebAæ•°æ®é›†å®Œæ•´æ€§"""
    print("\n=== CelebAæ•°æ®é›†å®Œæ•´æ€§æ£€æŸ¥ ===")
    
    base_dir = Path(r'D:\KKK\data\CelebA')
    annotations_dir = base_dir / 'annotations'
    images_dir = base_dir / 'img_align_celeba'
    
    # æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶
    annotation_files = [
        'list_attr_celeba.txt',
        'list_bbox_celeba.txt', 
        'list_eval_partition.txt'
    ]
    
    for filename in annotation_files:
        filepath = annotations_dir / filename
        info = check_file_exists_and_size(filepath)
        print(f"  {info['status']} {filename}: {info['size_mb']} MB")
        
        # æ£€æŸ¥æ–‡ä»¶å†…å®¹è¡Œæ•°
        if info['exists'] and info['size'] > 1000:
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    print(f"    ğŸ“Š åŒ…å« {len(lines)} è¡Œ")
            except Exception as e:
                print(f"    âš ï¸ è¯»å–é”™è¯¯: {e}")
    
    # æ£€æŸ¥å›¾åƒç›®å½•
    if images_dir.exists():
        image_files = list(images_dir.glob('*.jpg'))
        print(f"  ğŸ“ å›¾åƒç›®å½•: {len(image_files)} ä¸ªå›¾åƒæ–‡ä»¶")
        
        # æ£€æŸ¥é¢„æœŸçš„å›¾åƒæ•°é‡
        if len(image_files) >= 200000:
            print("    âœ… å›¾åƒæ•°é‡æ­£å¸¸ (>= 200k)")
        elif len(image_files) >= 100000:
            print("    âš ï¸ å›¾åƒæ•°é‡åå°‘ï¼Œä½†å¯ç”¨")
        else:
            print("    âŒ å›¾åƒæ•°é‡ä¸¥é‡ä¸è¶³")
    else:
        print("  âŒ å›¾åƒç›®å½•ä¸å­˜åœ¨")
    
    # æ£€æŸ¥zipæ–‡ä»¶
    zip_file = base_dir / 'img_align_celeba.zip'
    if zip_file.exists():
        zip_info = check_file_exists_and_size(zip_file)
        print(f"  ğŸ“¦ åŸå§‹zipæ–‡ä»¶: {zip_info['size_mb']} MB")

def generate_summary():
    """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
    print("\n=== æ•°æ®é›†çŠ¶æ€æ€»ç»“ ===")
    
    # VAWæ£€æŸ¥
    vaw_annotations = Path(r'D:\KKK\data\VAW\annotations')
    vaw_files = ['train_part1.json', 'train_part2.json', 'val.json', 'test.json']
    vaw_complete = all((vaw_annotations / f).exists() and 
                      (vaw_annotations / f).stat().st_size > 1000 
                      for f in vaw_files)
    
    # CelebAæ£€æŸ¥  
    celeba_annotations = Path(r'D:\KKK\data\CelebA\annotations')
    celeba_files = ['list_attr_celeba.txt', 'list_bbox_celeba.txt', 'list_eval_partition.txt']
    celeba_annotations_complete = all((celeba_annotations / f).exists() and 
                                    (celeba_annotations / f).stat().st_size > 1000 
                                    for f in celeba_files)
    
    celeba_images = Path(r'D:\KKK\data\CelebA\img_align_celeba')
    celeba_images_complete = celeba_images.exists() and len(list(celeba_images.glob('*.jpg'))) > 100000
    
    print(f"VAWæ•°æ®é›†æ ‡æ³¨: {'âœ… å®Œæ•´' if vaw_complete else 'âš ï¸ ä¸å®Œæ•´'}")
    print(f"CelebAæ•°æ®é›†æ ‡æ³¨: {'âœ… å®Œæ•´' if celeba_annotations_complete else 'âš ï¸ ä¸å®Œæ•´'}")  
    print(f"CelebAæ•°æ®é›†å›¾åƒ: {'âœ… å®Œæ•´' if celeba_images_complete else 'âš ï¸ ä¸å®Œæ•´'}")
    
    if vaw_complete and celeba_annotations_complete and celeba_images_complete:
        print("\nğŸ‰ æ‰€æœ‰æ•°æ®é›†éƒ½å·²å®Œæ•´å‡†å¤‡å¥½ï¼")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æ•°æ®é›†å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°è¯¦ç»†ä¿¡æ¯")

if __name__ == "__main__":
    print("å¼€å§‹æ•°æ®é›†å®Œæ•´æ€§æ£€æŸ¥...\n")
    
    check_vaw_dataset()
    check_celeba_dataset() 
    generate_summary()
    
    print("\næ£€æŸ¥å®Œæˆï¼") 