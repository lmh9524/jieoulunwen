#!/usr/bin/env python3
"""
CelebAæ•°æ®é›†è®­ç»ƒè„šæœ¬ - å¼±ç›‘ç£è§£è€¦çš„è·¨æ¨¡æ€å±æ€§å¯¹é½
"""

import os
import sys
import time
import torch
import torch.nn as nn
import torch.optim as optim
from datetime import datetime
import json
import numpy as np
from torch.cuda.amp import autocast, GradScaler

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('./weak_supervised_cross_modal')

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from config.base_config import get_config
from models import WeakSupervisedCrossModalAlignment
from training.losses import ComprehensiveLoss
from training.metrics import EvaluationMetrics
from data.celeba_dataset import CelebADatasetAdapter
from utils.logging_utils import setup_logging
from utils.checkpoint_utils import save_checkpoint, load_checkpoint

class CelebATrainer:
    """CelebAè®­ç»ƒå™¨"""
    
    def __init__(self, num_epochs=50, batch_size=16, learning_rate=1e-4):
        """
        åˆå§‹åŒ–CelebAè®­ç»ƒå™¨
        
        Args:
            num_epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹å¤„ç†å¤§å°
            learning_rate: å­¦ä¹ ç‡
        """
        # è·å–é…ç½®
        self.config = get_config('CelebA')
        self.config.num_epochs = num_epochs
        self.config.batch_size = batch_size
        self.config.learning_rate = learning_rate
        
        # è®¾å¤‡é…ç½®
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.config.device = self.device
        
        # åˆ›å»ºå®éªŒç›®å½•
        self.experiment_name = f"celeba_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.save_dir = f"./experiments/{self.experiment_name}"
        os.makedirs(self.save_dir, exist_ok=True)
        os.makedirs(f"{self.save_dir}/checkpoints", exist_ok=True)
        os.makedirs(f"{self.save_dir}/logs", exist_ok=True)
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.training_history = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': []
        }
        
        print("=" * 60)
        print("CelebA å¼±ç›‘ç£è§£è€¦è®­ç»ƒå™¨åˆå§‹åŒ–")
        print("=" * 60)
        print(f"è®¾å¤‡: {self.device}")
        print(f"è®­ç»ƒè½®æ•°: {num_epochs}")
        print(f"æ‰¹å¤„ç†å¤§å°: {batch_size}")
        print(f"å­¦ä¹ ç‡: {learning_rate}")
        print(f"å®éªŒç›®å½•: {self.save_dir}")
        
        if self.device.type == 'cuda':
            print(f"GPU: {torch.cuda.get_device_name()}")
            print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    def setup_data(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        print("\nè®¾ç½®æ•°æ®åŠ è½½å™¨...")
        
        adapter = CelebADatasetAdapter(self.config)
        self.dataloaders = adapter.get_dataloaders()
        
        # è·å–æ•°æ®é›†ä¿¡æ¯
        train_size = len(self.dataloaders['train'].dataset)
        val_size = len(self.dataloaders['val'].dataset)
        test_size = len(self.dataloaders['test'].dataset)
        
        print(f"è®­ç»ƒé›†: {train_size:,} æ ·æœ¬")
        print(f"éªŒè¯é›†: {val_size:,} æ ·æœ¬")
        print(f"æµ‹è¯•é›†: {test_size:,} æ ·æœ¬")
        
        return True
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        print("\nè®¾ç½®æ¨¡å‹...")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = WeakSupervisedCrossModalAlignment(self.config)
        self.model.to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        self.criterion = ComprehensiveLoss(self.config)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=self.config.num_epochs
        )
        
        # AMPæ··åˆç²¾åº¦
        self.scaler = GradScaler(enabled=(self.device.type == 'cuda'))
        
        # è¯„ä¼°æŒ‡æ ‡
        self.metrics = EvaluationMetrics(self.config.num_classes)
        
        return True
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_loss = 0.0
        total_samples = 0
        correct_predictions = {}
        total_predictions = {}
        
        # åˆå§‹åŒ–å‡†ç¡®ç‡ç»Ÿè®¡
        for key in self.config.num_classes.keys():
            correct_predictions[key] = 0
            total_predictions[key] = 0
        
        print(f"\nEpoch {epoch+1}/{self.config.num_epochs} - è®­ç»ƒé˜¶æ®µ")
        
        for batch_idx, (images, targets) in enumerate(self.dataloaders['train']):
            # æ•°æ®ç§»åˆ°è®¾å¤‡
            images = images.to(self.device)
            targets = {k: v.to(self.device) for k, v in targets.items()}
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad(set_to_none=True)
            
            with autocast(enabled=(self.device.type == 'cuda')):
                outputs = self.model(images)
                # è®¡ç®—æŸå¤±
                loss, loss_components = self.criterion(outputs, targets, epoch)
            
            # åå‘ä¼ æ’­ with AMP
            self.scaler.scale(loss).backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.scaler.step(self.optimizer)
            self.scaler.update()
            
            # ç»Ÿè®¡
            batch_size = images.size(0)
            total_loss += loss.item() * batch_size
            total_samples += batch_size
            
            # è®¡ç®—å‡†ç¡®ç‡
            for key in self.config.num_classes.keys():
                if 'predictions' in outputs and key in outputs['predictions'] and key in targets:
                    logits = outputs['predictions'][key]
                    pred = torch.argmax(logits, dim=1)
                    correct = (pred == targets[key]).sum().item()
                    correct_predictions[key] += correct
                    total_predictions[key] += batch_size
            
            # æ‰“å°è¿›åº¦
            if batch_idx % 100 == 0:
                progress = 100. * batch_idx / len(self.dataloaders['train'])
                print(f"  è¿›åº¦: {progress:.1f}%, æŸå¤±: {loss.item():.4f}")
        
        # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
        avg_loss = total_loss / total_samples
        avg_accuracy = {}
        for key in self.config.num_classes.keys():
            if total_predictions[key] > 0:
                avg_accuracy[key] = correct_predictions[key] / total_predictions[key]
        
        overall_accuracy = np.mean(list(avg_accuracy.values()))
        
        print(f"  è®­ç»ƒæŸå¤±: {avg_loss:.4f}")
        print(f"  è®­ç»ƒå‡†ç¡®ç‡: {overall_accuracy:.4f}")
        
        return avg_loss, overall_accuracy
    
    def validate_epoch(self, epoch):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        
        total_loss = 0.0
        total_samples = 0
        correct_predictions = {}
        total_predictions = {}
        
        # åˆå§‹åŒ–å‡†ç¡®ç‡ç»Ÿè®¡
        for key in self.config.num_classes.keys():
            correct_predictions[key] = 0
            total_predictions[key] = 0
        
        print(f"  éªŒè¯é˜¶æ®µ...")
        
        with torch.no_grad():
            for images, targets in self.dataloaders['val']:
                # æ•°æ®ç§»åˆ°è®¾å¤‡
                images = images.to(self.device)
                targets = {k: v.to(self.device) for k, v in targets.items()}
                
                # å‰å‘ä¼ æ’­
                with autocast(enabled=(self.device.type == 'cuda')):
                    outputs = self.model(images)
                    # è®¡ç®—æŸå¤±
                    loss, _ = self.criterion(outputs, targets, epoch)
                
                # ç»Ÿè®¡
                batch_size = images.size(0)
                total_loss += loss.item() * batch_size
                total_samples += batch_size
                
                # è®¡ç®—å‡†ç¡®ç‡
                for key in self.config.num_classes.keys():
                    if 'predictions' in outputs and key in outputs['predictions'] and key in targets:
                        logits = outputs['predictions'][key]
                        pred = torch.argmax(logits, dim=1)
                        correct = (pred == targets[key]).sum().item()
                        correct_predictions[key] += correct
                        total_predictions[key] += batch_size
        
        # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
        avg_loss = total_loss / total_samples
        avg_accuracy = {}
        for key in self.config.num_classes.keys():
            if total_predictions[key] > 0:
                avg_accuracy[key] = correct_predictions[key] / total_predictions[key]
        
        overall_accuracy = np.mean(list(avg_accuracy.values()))
        
        print(f"  éªŒè¯æŸå¤±: {avg_loss:.4f}")
        print(f"  éªŒè¯å‡†ç¡®ç‡: {overall_accuracy:.4f}")
        
        return avg_loss, overall_accuracy
    
    def save_checkpoint(self, epoch, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'training_history': self.training_history,
            'config': self.config
        }
        
        # ä¿å­˜å½“å‰æ£€æŸ¥ç‚¹
        checkpoint_path = f"{self.save_dir}/checkpoints/checkpoint_epoch_{epoch}.pth"
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = f"{self.save_dir}/checkpoints/best_model.pth"
            torch.save(checkpoint, best_path)
            print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
    
    def train(self):
        """æ‰§è¡Œå®Œæ•´è®­ç»ƒæµç¨‹"""
        print("\nå¼€å§‹CelebAè®­ç»ƒ...")
        
        # è®¾ç½®æ•°æ®å’Œæ¨¡å‹
        if not self.setup_data():
            return False
        
        if not self.setup_model():
            return False
        
        # ä¿å­˜é…ç½®
        config_path = f"{self.save_dir}/config.json"
        with open(config_path, 'w') as f:
            # å°†é…ç½®è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
            config_dict = {
                'dataset_name': self.config.dataset_name,
                'num_epochs': self.config.num_epochs,
                'batch_size': self.config.batch_size,
                'learning_rate': self.config.learning_rate,
                'image_size': self.config.image_size,
                'num_classes': self.config.num_classes,
                'loss_weights': self.config.loss_weights
            }
            json.dump(config_dict, f, indent=2)
        
        # è®­ç»ƒå¾ªç¯
        start_time = time.time()
        
        for epoch in range(self.config.num_epochs):
            print(f"\n{'='*60}")
            print(f"Epoch {epoch+1}/{self.config.num_epochs}")
            print(f"{'='*60}")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch)
            
            # éªŒè¯
            val_loss, val_acc = self.validate_epoch(epoch)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            
            # è®°å½•å†å²
            self.training_history['train_loss'].append(train_loss)
            self.training_history['val_loss'].append(val_loss)
            self.training_history['train_acc'].append(train_acc)
            self.training_history['val_acc'].append(val_acc)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
            is_best = val_loss < self.best_val_loss
            if is_best:
                self.best_val_loss = val_loss
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint(epoch, is_best)
            
            # æ‰“å°æ‘˜è¦
            elapsed = time.time() - start_time
            print(f"\nğŸ“Š Epoch {epoch+1} æ‘˜è¦:")
            print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
            print(f"  éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
            print(f"  æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
            print(f"  ç”¨æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        self.generate_training_report(total_time)
        
        return True
    
    def generate_training_report(self, total_time):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        print("\n" + "="*60)
        print("CelebA è®­ç»ƒå®ŒæˆæŠ¥å‘Š")
        print("="*60)
        
        print(f"æ€»è®­ç»ƒæ—¶é—´: {total_time/3600:.2f} å°æ—¶")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
        print(f"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {self.training_history['train_acc'][-1]:.4f}")
        print(f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {self.training_history['val_acc'][-1]:.4f}")
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = f"{self.save_dir}/training_history.json"
        with open(history_path, 'w') as f:
            json.dump(self.training_history, f, indent=2)
        
        # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
        report = {
            "experiment_name": self.experiment_name,
            "dataset": "CelebA",
            "total_epochs": self.config.num_epochs,
            "total_time_hours": total_time / 3600,
            "best_val_loss": self.best_val_loss,
            "final_train_acc": self.training_history['train_acc'][-1],
            "final_val_acc": self.training_history['val_acc'][-1],
            "model_parameters": sum(p.numel() for p in self.model.parameters()),
            "device": str(self.device),
            "config": {
                "batch_size": self.config.batch_size,
                "learning_rate": self.config.learning_rate,
                "num_classes": self.config.num_classes
            }
        }
        
        report_path = f"{self.save_dir}/training_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"\nğŸ“„ è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        print(f"ğŸ“„ è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")
        print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {self.save_dir}/checkpoints/best_model.pth")
        print("="*60)

def main():
    """ä¸»å‡½æ•°"""
    print("CelebA å¼±ç›‘ç£è§£è€¦è®­ç»ƒå¯åŠ¨")
    print("Copyright (c) 2024 - å¼±ç›‘ç£è§£è€¦çš„è·¨æ¨¡æ€å±æ€§å¯¹é½é¡¹ç›®")
    
    # æ£€æŸ¥CelebAæ•°æ®é›† (ç›¸å¯¹è·¯å¾„)
    celeba_path = ".."  # ç›¸å¯¹äºjieoulunwen-masterç›®å½•
    if not os.path.exists(celeba_path):
        print(f"âŒ é”™è¯¯: CelebAæ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {celeba_path}")
        return
    
    if not os.path.exists(f"{celeba_path}/img_align_celeba"):
        print(f"âŒ é”™è¯¯: CelebAå›¾åƒç›®å½•ä¸å­˜åœ¨: {os.path.abspath(celeba_path)}/img_align_celeba")
        return
        
    if not os.path.exists(f"{celeba_path}/Anno"):
        print(f"âŒ é”™è¯¯: CelebAæ ‡æ³¨ç›®å½•ä¸å­˜åœ¨: {os.path.abspath(celeba_path)}/Anno")
        return
        
    if not os.path.exists(f"{celeba_path}/Eval"):
        print(f"âŒ é”™è¯¯: CelebAè¯„ä¼°ç›®å½•ä¸å­˜åœ¨: {os.path.abspath(celeba_path)}/Eval")
        return
    
    print(f"âœ… CelebAæ•°æ®é›†æ£€æŸ¥é€šè¿‡: {celeba_path}")
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = CelebATrainer(
        num_epochs=50,
        batch_size=16,
        learning_rate=1e-4
    )
    
    success = trainer.train()
    
    if success:
        print("\nğŸ‰ CelebAè®­ç»ƒæˆåŠŸå®Œæˆ!")
    else:
        print("\nâŒ CelebAè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")

if __name__ == "__main__":
    main() 