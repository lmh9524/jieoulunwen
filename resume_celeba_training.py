#!/usr/bin/env python3
"""
CelebAè®­ç»ƒæ¢å¤è„šæœ¬ - ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ
"""

import os
import sys
import time
import torch
import torch.nn as nn
import torch.optim as optim
from datetime import datetime
import json
import numpy as np
from torch.cuda.amp import autocast, GradScaler
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('./weak_supervised_cross_modal')

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from config.base_config import get_config
from models import WeakSupervisedCrossModalAlignment
from training.losses import ComprehensiveLoss
from training.metrics import EvaluationMetrics
from data.celeba_dataset import CelebADatasetAdapter
from utils.logging_utils import setup_logging
from utils.checkpoint_utils import load_checkpoint

class CelebATrainerResume:
    """CelebAæ¢å¤è®­ç»ƒå™¨"""
    
    def __init__(self, checkpoint_path, start_epoch=None, total_epochs=50):
        """
        åˆå§‹åŒ–æ¢å¤è®­ç»ƒå™¨
        
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
            start_epoch: æŒ‡å®šå¼€å§‹è½®æ¬¡ï¼ˆå¯é€‰ï¼‰
            total_epochs: æ€»è®­ç»ƒè½®æ•°
        """
        print("="*60)
        print("CelebA æ¢å¤è®­ç»ƒå™¨åˆå§‹åŒ–")
        print("="*60)
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        print(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        # è®¾å¤‡é…ç½®
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"è®¾å¤‡: {self.device}")
        
        if self.device.type == 'cuda':
            print(f"GPU: {torch.cuda.get_device_name()}")
            print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        
        # åŠ è½½æ£€æŸ¥ç‚¹æ•°æ®
        self.checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # ä»æ£€æŸ¥ç‚¹æ¢å¤é…ç½®
        if 'config' in self.checkpoint:
            self.config = self.checkpoint['config']
        else:
            # å¦‚æœæ£€æŸ¥ç‚¹ä¸­æ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
            self.config = get_config('CelebA')
        
        self.config.device = self.device
        self.config.num_epochs = total_epochs
        
        # ç¡®å®šå¼€å§‹è½®æ¬¡
        self.start_epoch = start_epoch if start_epoch is not None else self.checkpoint.get('epoch', 0) + 1
        self.best_val_loss = self.checkpoint.get('best_val_loss', float('inf'))
        
        # æ¢å¤è®­ç»ƒå†å²
        self.training_history = self.checkpoint.get('training_history', {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': []
        })
        
        # åˆ›å»ºæ–°çš„å®éªŒç›®å½•
        self.experiment_name = f"celeba_resume_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.save_dir = f"./experiments/{self.experiment_name}"
        os.makedirs(self.save_dir, exist_ok=True)
        os.makedirs(f"{self.save_dir}/checkpoints", exist_ok=True)
        os.makedirs(f"{self.save_dir}/logs", exist_ok=True)
        
        print(f"å¼€å§‹è½®æ¬¡: {self.start_epoch}")
        print(f"æ€»è½®æ•°: {total_epochs}")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
        print(f"æ–°å®éªŒç›®å½•: {self.save_dir}")
        
        # åˆå§‹åŒ–AMP
        self.scaler = GradScaler()
    
    def setup_data(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        print("\nè®¾ç½®æ•°æ®åŠ è½½å™¨...")
        
        adapter = CelebADatasetAdapter(self.config)
        self.dataloaders = adapter.get_dataloaders()
        
        # è·å–æ•°æ®é›†ä¿¡æ¯
        train_size = len(self.dataloaders['train'].dataset)
        val_size = len(self.dataloaders['val'].dataset)
        test_size = len(self.dataloaders['test'].dataset)
        
        print(f"è®­ç»ƒé›†: {train_size:,} æ ·æœ¬")
        print(f"éªŒè¯é›†: {val_size:,} æ ·æœ¬") 
        print(f"æµ‹è¯•é›†: {test_size:,} æ ·æœ¬")
        
        return True
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œä¼˜åŒ–å™¨"""
        print("\nè®¾ç½®æ¨¡å‹...")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = WeakSupervisedCrossModalAlignment(self.config)
        self.model.to(self.device)
        
        # åŠ è½½æ¨¡å‹æƒé‡
        self.model.load_state_dict(self.checkpoint['model_state_dict'])
        print("âœ… æ¨¡å‹æƒé‡å·²æ¢å¤")
        
        # è®¾ç½®æŸå¤±å‡½æ•°
        self.criterion = ComprehensiveLoss(self.config)
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=0.01
        )
        
        # æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€
        if 'optimizer_state_dict' in self.checkpoint:
            self.optimizer.load_state_dict(self.checkpoint['optimizer_state_dict'])
            print("âœ… ä¼˜åŒ–å™¨çŠ¶æ€å·²æ¢å¤")
        
        # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.StepLR(
            self.optimizer,
            step_size=10,
            gamma=0.1
        )
        
        # æ¢å¤è°ƒåº¦å™¨çŠ¶æ€
        if 'scheduler_state_dict' in self.checkpoint:
            self.scheduler.load_state_dict(self.checkpoint['scheduler_state_dict'])
            print("âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€å·²æ¢å¤")
        
        # è®¾ç½®è¯„ä¼°æŒ‡æ ‡
        self.metrics = EvaluationMetrics(self.config.num_classes)
        
        # æ¨¡å‹å‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"æ¨¡å‹æ€»å‚æ•°: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        return True
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        running_loss = 0.0
        correct_predictions = {key: 0 for key in self.config.num_classes.keys()}
        total_predictions = {key: 0 for key in self.config.num_classes.keys()}
        
        dataloader = self.dataloaders['train']
        total_batches = len(dataloader)
        
        print(f"Epoch {epoch+1}/{self.config.num_epochs} - è®­ç»ƒé˜¶æ®µ")
        
        for batch_idx, batch in enumerate(dataloader):
            # æ•°æ®ç§»åˆ°è®¾å¤‡
            images = batch['image'].to(self.device)
            targets = {key: batch[key].to(self.device) for key in self.config.num_classes.keys()}
            
            # å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨æ··åˆç²¾åº¦ï¼‰
            with autocast():
                outputs = self.model(images, targets)
                loss = self.criterion(outputs, targets)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()
            
            # ç»Ÿè®¡
            running_loss += loss.item()
            
            # è®¡ç®—å‡†ç¡®ç‡
            for key in self.config.num_classes.keys():
                if key in outputs['predictions']:
                    predictions = outputs['predictions'][key].argmax(dim=1)
                    correct_predictions[key] += (predictions == targets[key]).sum().item()
                    total_predictions[key] += targets[key].size(0)
            
            # æ‰“å°è¿›åº¦
            if batch_idx % 100 == 0:
                progress = 100.0 * batch_idx / total_batches
                current_loss = running_loss / (batch_idx + 1)
                print(f"  è¿›åº¦: {progress:.1f}% æŸå¤±: {current_loss:.4f}")
        
        # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
        avg_loss = running_loss / total_batches
        avg_accuracy = {}
        for key in self.config.num_classes.keys():
            if total_predictions[key] > 0:
                avg_accuracy[key] = correct_predictions[key] / total_predictions[key]
        
        overall_accuracy = np.mean(list(avg_accuracy.values()))
        
        print(f"  è®­ç»ƒæŸå¤±: {avg_loss:.4f}")
        print(f"  è®­ç»ƒå‡†ç¡®ç‡: {overall_accuracy:.4f}")
        
        return avg_loss, overall_accuracy
    
    def validate_epoch(self, epoch):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        running_loss = 0.0
        correct_predictions = {key: 0 for key in self.config.num_classes.keys()}
        total_predictions = {key: 0 for key in self.config.num_classes.keys()}
        
        dataloader = self.dataloaders['val']
        total_batches = len(dataloader)
        
        print(f"Epoch {epoch+1}/{self.config.num_epochs} - éªŒè¯é˜¶æ®µ")
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(dataloader):
                # æ•°æ®ç§»åˆ°è®¾å¤‡
                images = batch['image'].to(self.device)
                targets = {key: batch[key].to(self.device) for key in self.config.num_classes.keys()}
                
                # å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨æ··åˆç²¾åº¦ï¼‰
                with autocast():
                    outputs = self.model(images, targets)
                    loss = self.criterion(outputs, targets)
                
                # ç»Ÿè®¡
                running_loss += loss.item()
                
                # è®¡ç®—å‡†ç¡®ç‡
                for key in self.config.num_classes.keys():
                    if key in outputs['predictions']:
                        predictions = outputs['predictions'][key].argmax(dim=1)
                        correct_predictions[key] += (predictions == targets[key]).sum().item()
                        total_predictions[key] += targets[key].size(0)
        
        # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
        avg_loss = running_loss / total_batches
        avg_accuracy = {}
        for key in self.config.num_classes.keys():
            if total_predictions[key] > 0:
                avg_accuracy[key] = correct_predictions[key] / total_predictions[key]
        
        overall_accuracy = np.mean(list(avg_accuracy.values()))
        
        print(f"  éªŒè¯æŸå¤±: {avg_loss:.4f}")
        print(f"  éªŒè¯å‡†ç¡®ç‡: {overall_accuracy:.4f}")
        
        return avg_loss, overall_accuracy
    
    def save_checkpoint(self, epoch, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'training_history': self.training_history,
            'config': self.config
        }
        
        # ä¿å­˜å½“å‰æ£€æŸ¥ç‚¹
        checkpoint_path = f"{self.save_dir}/checkpoints/checkpoint_epoch_{epoch}.pth"
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = f"{self.save_dir}/checkpoints/best_model.pth"
            torch.save(checkpoint, best_path)
            print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
    
    def resume_train(self):
        """æ‰§è¡Œæ¢å¤è®­ç»ƒ"""
        print(f"\nğŸ”„ ä»ç¬¬ {self.start_epoch} è½®æ¢å¤CelebAè®­ç»ƒ...")
        
        # è®¾ç½®æ•°æ®å’Œæ¨¡å‹
        if not self.setup_data():
            return False
        
        if not self.setup_model():
            return False
        
        # ä¿å­˜æ¢å¤é…ç½®
        config_path = f"{self.save_dir}/resume_config.json"
        with open(config_path, 'w') as f:
            config_dict = {
                'dataset_name': self.config.dataset_name,
                'num_epochs': self.config.num_epochs,
                'start_epoch': self.start_epoch,
                'batch_size': self.config.batch_size,
                'learning_rate': self.config.learning_rate,
                'image_size': self.config.image_size,
                'num_classes': self.config.num_classes,
                'loss_weights': self.config.loss_weights,
                'resumed_from': self.checkpoint.get('epoch', 'unknown')
            }
            json.dump(config_dict, f, indent=2)
        
        # è®­ç»ƒå¾ªç¯
        start_time = time.time()
        
        for epoch in range(self.start_epoch, self.config.num_epochs):
            print(f"\n{'='*60}")
            print(f"Epoch {epoch+1}/{self.config.num_epochs} (æ¢å¤è®­ç»ƒ)")
            print(f"{'='*60}")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch)
            
            # éªŒè¯
            val_loss, val_acc = self.validate_epoch(epoch)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            
            # è®°å½•å†å²
            self.training_history['train_loss'].append(train_loss)
            self.training_history['val_loss'].append(val_loss)
            self.training_history['train_acc'].append(train_acc)
            self.training_history['val_acc'].append(val_acc)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
            is_best = val_loss < self.best_val_loss
            if is_best:
                self.best_val_loss = val_loss
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint(epoch, is_best)
            
            # æ‰“å°æ‘˜è¦
            elapsed = time.time() - start_time
            print(f"\nğŸ“Š Epoch {epoch+1} æ‘˜è¦:")
            print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
            print(f"  éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
            print(f"  æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
            print(f"  æ¢å¤è®­ç»ƒç”¨æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        self.generate_training_report(total_time)
        
        return True
    
    def generate_training_report(self, total_time):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        print("\n" + "="*60)
        print("CelebA æ¢å¤è®­ç»ƒå®ŒæˆæŠ¥å‘Š")
        print("="*60)
        
        print(f"æ¢å¤è®­ç»ƒæ—¶é—´: {total_time/3600:.2f} å°æ—¶")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
        print(f"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {self.training_history['train_acc'][-1]:.4f}")
        print(f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {self.training_history['val_acc'][-1]:.4f}")
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = f"{self.save_dir}/training_history.json"
        with open(history_path, 'w') as f:
            json.dump(self.training_history, f, indent=2)
        
        print(f"\nğŸ“„ æ¢å¤è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")
        print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {self.save_dir}/checkpoints/best_model.pth")
        print("="*60)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='CelebAæ¢å¤è®­ç»ƒè„šæœ¬')
    parser.add_argument('--checkpoint', type=str, required=True, help='æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--start_epoch', type=int, default=None, help='æŒ‡å®šå¼€å§‹è½®æ¬¡ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--total_epochs', type=int, default=50, help='æ€»è®­ç»ƒè½®æ•°')
    
    args = parser.parse_args()
    
    print("CelebA æ¢å¤è®­ç»ƒå¯åŠ¨")
    print("Copyright (c) 2024 - å¼±ç›‘ç£è§£è€¦çš„è·¨æ¨¡æ€å±æ€§å¯¹é½é¡¹ç›®")
    
    # æ£€æŸ¥æ£€æŸ¥ç‚¹æ–‡ä»¶
    if not os.path.exists(args.checkpoint):
        print(f"âŒ é”™è¯¯: æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {args.checkpoint}")
        return
    
    print(f"âœ… æ£€æŸ¥ç‚¹æ–‡ä»¶æ£€æŸ¥é€šè¿‡: {args.checkpoint}")
    
    # åˆ›å»ºæ¢å¤è®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = CelebATrainerResume(
        checkpoint_path=args.checkpoint,
        start_epoch=args.start_epoch,
        total_epochs=args.total_epochs
    )
    
    success = trainer.resume_train()
    
    if success:
        print("\nğŸ‰ CelebAæ¢å¤è®­ç»ƒæˆåŠŸå®Œæˆ!")
    else:
        print("\nâŒ CelebAæ¢å¤è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")

if __name__ == "__main__":
    main() 