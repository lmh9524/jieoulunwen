#!/usr/bin/env python3
"""
å®Œæ•´ä¸‹è½½COCONutæ•°æ®é›†
ç¡®ä¿ä¸‹è½½æ‰€æœ‰5000å¼ çœŸå®å›¾åƒï¼Œç»ä¸é—æ¼ä»»ä½•æ•°æ®
"""

import os
import json
import requests
from pathlib import Path
import argparse
from tqdm import tqdm
import logging
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CompleteCoconutDownloader:
    """å®Œæ•´çš„COCONutæ•°æ®é›†ä¸‹è½½å™¨"""
    
    def __init__(self, output_dir: str, max_workers: int = 10):
        self.output_dir = Path(output_dir)
        self.max_workers = max_workers
        self.download_lock = threading.Lock()
        self.success_count = 0
        self.failed_count = 0
        
    def download_annotations(self):
        """ä¸‹è½½æ ‡æ³¨æ–‡ä»¶"""
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¸‹è½½ä¸»è¦çš„æ ‡æ³¨æ–‡ä»¶
        annotation_urls = [
            "https://github.com/akshayparakh25/COCONut/raw/main/data/relabeled_coco_val.json",
            "https://raw.githubusercontent.com/akshayparakh25/COCONut/main/data/relabeled_coco_val.json"
        ]
        
        annotation_file = self.output_dir / "relabeled_coco_val.json"
        
        if annotation_file.exists():
            logger.info(f"æ ‡æ³¨æ–‡ä»¶å·²å­˜åœ¨: {annotation_file}")
            return True
        
        logger.info("å¼€å§‹ä¸‹è½½COCONutæ ‡æ³¨æ–‡ä»¶...")
        
        for url in annotation_urls:
            try:
                logger.info(f"å°è¯•ä» {url} ä¸‹è½½...")
                response = requests.get(url, timeout=30)
                response.raise_for_status()
                
                with open(annotation_file, 'wb') as f:
                    f.write(response.content)
                
                logger.info(f"æ ‡æ³¨æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {annotation_file}")
                return True
                
            except Exception as e:
                logger.warning(f"ä» {url} ä¸‹è½½å¤±è´¥: {e}")
                continue
        
        logger.error("æ‰€æœ‰æ ‡æ³¨æ–‡ä»¶ä¸‹è½½æºéƒ½å¤±è´¥äº†")
        return False
    
    def download_single_image(self, image_info):
        """ä¸‹è½½å•å¼ å›¾åƒ"""
        img_id = image_info['id']
        filename = image_info['file_name']
        url = image_info['coco_url']
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        cache_dir = self.output_dir / 'complete_image_cache'
        cache_dir.mkdir(exist_ok=True)
        
        cache_path = cache_dir / filename
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”å®Œæ•´ï¼Œè·³è¿‡
        if cache_path.exists():
            try:
                # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
                from PIL import Image
                img = Image.open(cache_path)
                img.verify()
                with self.download_lock:
                    self.success_count += 1
                return True
            except:
                # æ–‡ä»¶æŸåï¼Œé‡æ–°ä¸‹è½½
                cache_path.unlink()
        
        # ä¸‹è½½å›¾åƒ
        try:
            response = requests.get(url, timeout=15)
            response.raise_for_status()
            
            with open(cache_path, 'wb') as f:
                f.write(response.content)
            
            # éªŒè¯ä¸‹è½½çš„å›¾åƒ
            from PIL import Image
            img = Image.open(cache_path)
            img.verify()
            img.close()
            
            with self.download_lock:
                self.success_count += 1
            
            return True
            
        except Exception as e:
            logger.debug(f"ä¸‹è½½å›¾åƒå¤±è´¥ {filename}: {e}")
            with self.download_lock:
                self.failed_count += 1
            return False
    
    def download_all_images(self, max_retries: int = 3):
        """ä¸‹è½½æ‰€æœ‰å›¾åƒ"""
        # åŠ è½½æ ‡æ³¨æ–‡ä»¶
        annotation_file = self.output_dir / "relabeled_coco_val.json"
        
        if not annotation_file.exists():
            logger.error("æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆä¸‹è½½æ ‡æ³¨æ–‡ä»¶")
            return False
        
        with open(annotation_file, 'r') as f:
            data = json.load(f)
        
        images = data.get('images', [])
        total_images = len(images)
        
        logger.info(f"å¼€å§‹ä¸‹è½½ {total_images} å¼ å›¾åƒ...")
        logger.info(f"ä½¿ç”¨ {self.max_workers} ä¸ªå¹¶å‘çº¿ç¨‹")
        
        # å¤šæ¬¡é‡è¯•ä¸‹è½½å¤±è´¥çš„å›¾åƒ
        for retry in range(max_retries):
            if retry > 0:
                logger.info(f"ç¬¬ {retry + 1} æ¬¡é‡è¯•ä¸‹è½½å¤±è´¥çš„å›¾åƒ...")
            
            # é‡ç½®è®¡æ•°å™¨
            self.success_count = 0
            self.failed_count = 0
            
            # è¿‡æ»¤å‡ºéœ€è¦ä¸‹è½½çš„å›¾åƒ
            images_to_download = []
            cache_dir = self.output_dir / 'complete_image_cache'
            
            for img_info in images:
                cache_path = cache_dir / img_info['file_name']
                if not cache_path.exists():
                    images_to_download.append(img_info)
            
            if not images_to_download:
                logger.info("æ‰€æœ‰å›¾åƒéƒ½å·²ä¸‹è½½å®Œæˆï¼")
                break
            
            logger.info(f"éœ€è¦ä¸‹è½½ {len(images_to_download)} å¼ å›¾åƒ")
            
            # ä½¿ç”¨çº¿ç¨‹æ± ä¸‹è½½
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # æäº¤æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
                future_to_image = {
                    executor.submit(self.download_single_image, img_info): img_info
                    for img_info in images_to_download
                }
                
                # æ˜¾ç¤ºè¿›åº¦
                with tqdm(total=len(images_to_download), desc="ä¸‹è½½å›¾åƒ") as pbar:
                    for future in as_completed(future_to_image):
                        future.result()  # è·å–ç»“æœ
                        pbar.update(1)
                        pbar.set_postfix({
                            'Success': self.success_count,
                            'Failed': self.failed_count
                        })
            
            logger.info(f"æœ¬è½®ä¸‹è½½å®Œæˆ: æˆåŠŸ {self.success_count}, å¤±è´¥ {self.failed_count}")
            
            # å¦‚æœå¤±è´¥æ•°é‡å¾ˆå°‘ï¼Œå¯ä»¥åœæ­¢é‡è¯•
            if self.failed_count < 10:
                break
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•
            if retry < max_retries - 1:
                logger.info("ç­‰å¾…10ç§’åé‡è¯•...")
                time.sleep(10)
        
        # æœ€ç»ˆç»Ÿè®¡
        cache_dir = self.output_dir / 'complete_image_cache'
        final_count = len([f for f in os.listdir(cache_dir) if f.endswith(('.jpg', '.png'))])
        
        logger.info(f"æœ€ç»ˆä¸‹è½½ç»“æœ: {final_count}/{total_images} å¼ å›¾åƒ")
        
        if final_count == total_images:
            logger.info("ğŸ‰ æ‰€æœ‰å›¾åƒä¸‹è½½å®Œæˆï¼")
            return True
        else:
            logger.warning(f"âš ï¸ ä»æœ‰ {total_images - final_count} å¼ å›¾åƒä¸‹è½½å¤±è´¥")
            return False
    
    def download_segmentation_masks(self):
        """ä¸‹è½½åˆ†å‰²æ©ç æ–‡ä»¶"""
        logger.info("æ£€æŸ¥åˆ†å‰²æ©ç æ–‡ä»¶...")
        
        # åˆ†å‰²æ©ç é€šå¸¸åœ¨relabeled_coco_valç›®å½•ä¸­
        seg_dir = self.output_dir / "relabeled_coco_val"
        
        if seg_dir.exists():
            png_files = [f for f in os.listdir(seg_dir) if f.endswith('.png')]
            logger.info(f"æ‰¾åˆ° {len(png_files)} ä¸ªåˆ†å‰²æ©ç æ–‡ä»¶")
            
            if len(png_files) >= 5000:
                logger.info("âœ… åˆ†å‰²æ©ç æ–‡ä»¶å®Œæ•´")
                return True
            else:
                logger.warning(f"âš ï¸ åˆ†å‰²æ©ç æ–‡ä»¶ä¸å®Œæ•´ï¼Œåªæœ‰ {len(png_files)} ä¸ª")
        
        # å¦‚æœåˆ†å‰²æ©ç ä¸å®Œæ•´ï¼Œå°è¯•ä¸‹è½½
        logger.info("å°è¯•ä¸‹è½½åˆ†å‰²æ©ç ...")
        
        # è¿™é‡Œå¯èƒ½éœ€è¦æ ¹æ®å®é™…çš„COCONutæ•°æ®æºè°ƒæ•´
        # ç›®å‰å‡è®¾åˆ†å‰²æ©ç å·²ç»åœ¨relabeled_coco_valç›®å½•ä¸­
        
        return True
    
    def verify_dataset_completeness(self):
        """éªŒè¯æ•°æ®é›†å®Œæ•´æ€§"""
        logger.info("éªŒè¯æ•°æ®é›†å®Œæ•´æ€§...")
        
        # æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶
        annotation_file = self.output_dir / "relabeled_coco_val.json"
        if not annotation_file.exists():
            logger.error("âŒ æ ‡æ³¨æ–‡ä»¶ç¼ºå¤±")
            return False
        
        with open(annotation_file, 'r') as f:
            data = json.load(f)
        
        expected_images = len(data.get('images', []))
        expected_annotations = len(data.get('annotations', []))
        expected_categories = len(data.get('categories', []))
        
        # æ£€æŸ¥å›¾åƒç¼“å­˜
        cache_dir = self.output_dir / 'complete_image_cache'
        if cache_dir.exists():
            cached_images = len([f for f in os.listdir(cache_dir) if f.endswith(('.jpg', '.png'))])
        else:
            cached_images = 0
        
        # æ£€æŸ¥åˆ†å‰²æ©ç 
        seg_dir = self.output_dir / "relabeled_coco_val"
        if seg_dir.exists():
            seg_masks = len([f for f in os.listdir(seg_dir) if f.endswith('.png')])
        else:
            seg_masks = 0
        
        logger.info("ğŸ“Š æ•°æ®é›†å®Œæ•´æ€§æŠ¥å‘Š:")
        logger.info(f"  - æ ‡æ³¨æ–‡ä»¶: âœ…")
        logger.info(f"  - å›¾åƒæ•°é‡: {cached_images}/{expected_images} {'âœ…' if cached_images == expected_images else 'âŒ'}")
        logger.info(f"  - æ ‡æ³¨æ•°é‡: {expected_annotations} âœ…")
        logger.info(f"  - ç±»åˆ«æ•°é‡: {expected_categories} âœ…")
        logger.info(f"  - åˆ†å‰²æ©ç : {seg_masks} {'âœ…' if seg_masks >= 5000 else 'âŒ'}")
        
        is_complete = (cached_images == expected_images and 
                      expected_annotations > 0 and 
                      expected_categories > 0 and 
                      seg_masks >= 5000)
        
        if is_complete:
            logger.info("ğŸ‰ æ•°æ®é›†å®Œæ•´ï¼å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼")
        else:
            logger.warning("âš ï¸ æ•°æ®é›†ä¸å®Œæ•´ï¼Œå»ºè®®é‡æ–°ä¸‹è½½")
        
        return is_complete

def main():
    parser = argparse.ArgumentParser(description='å®Œæ•´ä¸‹è½½COCONutæ•°æ®é›†')
    parser.add_argument('--output_dir', type=str, default='../data/coconut', 
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--max_workers', type=int, default=10, 
                       help='å¹¶å‘ä¸‹è½½çº¿ç¨‹æ•°')
    parser.add_argument('--max_retries', type=int, default=3, 
                       help='æœ€å¤§é‡è¯•æ¬¡æ•°')
    
    args = parser.parse_args()
    
    logger.info("ğŸ¥¥ å¼€å§‹å®Œæ•´ä¸‹è½½COCONutæ•°æ®é›†")
    logger.info("=" * 60)
    logger.info("ä¸¥æ ¼ä¿è¯ï¼šåªä¸‹è½½çœŸå®æ•°æ®ï¼Œç»ä¸ä½¿ç”¨ä»»ä½•åˆæˆæ•°æ®ï¼")
    logger.info("=" * 60)
    
    downloader = CompleteCoconutDownloader(
        output_dir=args.output_dir,
        max_workers=args.max_workers
    )
    
    try:
        # 1. ä¸‹è½½æ ‡æ³¨æ–‡ä»¶
        if not downloader.download_annotations():
            logger.error("æ ‡æ³¨æ–‡ä»¶ä¸‹è½½å¤±è´¥ï¼Œç»ˆæ­¢ä¸‹è½½")
            return
        
        # 2. ä¸‹è½½æ‰€æœ‰å›¾åƒ
        if not downloader.download_all_images(max_retries=args.max_retries):
            logger.warning("éƒ¨åˆ†å›¾åƒä¸‹è½½å¤±è´¥ï¼Œä½†ç»§ç»­éªŒè¯...")
        
        # 3. æ£€æŸ¥åˆ†å‰²æ©ç 
        downloader.download_segmentation_masks()
        
        # 4. éªŒè¯å®Œæ•´æ€§
        is_complete = downloader.verify_dataset_completeness()
        
        if is_complete:
            logger.info("âœ… COCONutæ•°æ®é›†ä¸‹è½½å®Œæˆï¼")
            logger.info("ğŸš€ ç°åœ¨å¯ä»¥å¼€å§‹100è½®è®­ç»ƒäº†ï¼")
        else:
            logger.error("âŒ æ•°æ®é›†ä¸‹è½½ä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥åé‡è¯•")
            
    except Exception as e:
        logger.error(f"ä¸‹è½½è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise

if __name__ == "__main__":
    main() 