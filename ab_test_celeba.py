#!/usr/bin/env python3
"""
CelebA A/Bæµ‹è¯•è„šæœ¬ - å¯¹æ¯”baselineä¸ä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½
æ”¯æŒä¸€é”®è¯„ä¼°ã€ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒå’Œè¯¦ç»†å¯¹æ¯”æŠ¥å‘Š
"""

import os
import sys
import json
import time
import torch
import numpy as np
from datetime import datetime
from scipy import stats
from typing import Dict, Tuple, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('./weak_supervised_cross_modal')

from config.base_config import get_config
from config.celeba_optimized_config import get_optimized_config
from models import WeakSupervisedCrossModalAlignment
from training.losses import ComprehensiveLoss
from training.metrics import EvaluationMetrics
from data.celeba_dataset import CelebADatasetAdapter
from data.celeba_optimized_dataset import CelebAOptimizedDatasetAdapter

class ABTester:
    """A/Bæµ‹è¯•å™¨"""
    
    def __init__(self, baseline_checkpoint: str, optimized_checkpoint: str,
                 data_path: str = 'D:\\KKK\\data\\CelebA', batch_size: int = 32):
        """
        åˆå§‹åŒ–A/Bæµ‹è¯•å™¨
        
        Args:
            baseline_checkpoint: baselineæ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
            optimized_checkpoint: ä¼˜åŒ–æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
            data_path: CelebAæ•°æ®é›†è·¯å¾„
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        self.baseline_checkpoint = baseline_checkpoint
        self.optimized_checkpoint = optimized_checkpoint
        self.data_path = data_path
        self.batch_size = batch_size
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åŠ è½½é…ç½®
        self.baseline_config = get_config('CelebA')
        self.optimized_config = get_optimized_config(1)  # å‡è®¾æµ‹è¯•Stage 1
        
        # è°ƒæ•´é…ç½®
        self.baseline_config.data_path = data_path
        self.baseline_config.batch_size = batch_size
        self.optimized_config.data_path = data_path
        self.optimized_config.batch_size = batch_size
        
        print(f"ğŸ”¬ A/Bæµ‹è¯•åˆå§‹åŒ–")
        print(f"  è®¾å¤‡: {self.device}")
        print(f"  æ•°æ®é›†: {data_path}")
        print(f"  æ‰¹å¤„ç†: {batch_size}")
        
    def load_model(self, checkpoint_path: str, config, model_type: str):
        """åŠ è½½æ¨¡å‹"""
        print(f"\nğŸ“‚ åŠ è½½{model_type}æ¨¡å‹: {os.path.basename(checkpoint_path)}")
        
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
        
        # åˆ›å»ºæ¨¡å‹
        model = WeakSupervisedCrossModalAlignment(config)
        model.to(self.device)
        
        # åŠ è½½æƒé‡
        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        else:
            model.load_state_dict(checkpoint, strict=False)
        
        model.eval()
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters())
        print(f"  å‚æ•°æ•°é‡: {total_params:,}")
        
        # æ‰“å°æ£€æŸ¥ç‚¹ä¿¡æ¯
        if 'epoch' in checkpoint:
            print(f"  è®­ç»ƒè½®æ¬¡: {checkpoint['epoch']}")
        if 'best_val_loss' in checkpoint:
            print(f"  æœ€ä½³éªŒè¯æŸå¤±: {checkpoint['best_val_loss']:.4f}")
        
        return model
    
    def setup_dataloaders(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        print(f"\nğŸ”„ è®¾ç½®æ•°æ®åŠ è½½å™¨...")
        
        # Baselineæ•°æ®åŠ è½½å™¨
        baseline_adapter = CelebADatasetAdapter(self.baseline_config)
        baseline_dataloaders = baseline_adapter.get_dataloaders()
        
        # ä¼˜åŒ–æ•°æ®åŠ è½½å™¨
        optimized_adapter = CelebAOptimizedDatasetAdapter(self.optimized_config)
        optimized_dataloaders = optimized_adapter.get_dataloaders()
        
        print(f"  æµ‹è¯•é›†æ ·æœ¬: {len(baseline_dataloaders['test'].dataset):,}")
        
        return baseline_dataloaders['test'], optimized_dataloaders['test']
    
    def evaluate_model(self, model, dataloader, config, model_name: str) -> Dict:
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        print(f"\nğŸ§ª è¯„ä¼°{model_name}æ¨¡å‹...")
        
        model.eval()
        
        # åˆå§‹åŒ–æŒ‡æ ‡
        metrics = EvaluationMetrics(config.num_classes)
        criterion = ComprehensiveLoss(config)
        
        total_loss = 0.0
        total_samples = 0
        predictions_list = []
        targets_list = []
        
        start_time = time.time()
        
        with torch.no_grad():
            for batch_idx, (images, targets) in enumerate(dataloader):
                images = images.to(self.device)
                targets = {k: v.to(self.device) for k, v in targets.items()}
                
                # å‰å‘ä¼ æ’­
                outputs = model(images)
                loss, _ = criterion(outputs, targets, epoch=0)
                
                # ç»Ÿè®¡
                batch_size = images.size(0)
                total_loss += loss.item() * batch_size
                total_samples += batch_size
                
                # æ”¶é›†é¢„æµ‹å’Œç›®æ ‡
                if 'predictions' in outputs:
                    predictions_list.append({k: v.cpu() for k, v in outputs['predictions'].items()})
                    targets_list.append({k: v.cpu() for k, v in targets.items()})
                    
                    # æ›´æ–°æŒ‡æ ‡
                    metrics.update(outputs['predictions'], targets)
                
                # è¿›åº¦æ˜¾ç¤º
                if batch_idx % 20 == 0:
                    progress = 100. * batch_idx / len(dataloader)
                    print(f"    è¿›åº¦: {progress:.1f}%")
        
        elapsed = time.time() - start_time
        avg_loss = total_loss / max(1, total_samples)
        metric_results = metrics.compute()
        
        print(f"  å®Œæˆè¯„ä¼°ï¼Œç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ")
        print(f"  å¹³å‡æŸå¤±: {avg_loss:.4f}")
        print(f"  å¹³å‡å‡†ç¡®ç‡: {metric_results.get('mean_accuracy', 0.0):.4f}")
        
        return {
            'avg_loss': avg_loss,
            'metrics': metric_results,
            'elapsed': elapsed,
            'predictions': predictions_list,
            'targets': targets_list
        }
    
    def compute_statistical_significance(self, baseline_results: Dict, 
                                       optimized_results: Dict) -> Dict:
        """è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§"""
        print(f"\nğŸ“Š è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§...")
        
        significance_results = {}
        
        # å¯¹æ¯ä¸ªå±æ€§ç»„è¿›è¡Œtæ£€éªŒ
        for attr in self.baseline_config.num_classes.keys():
            if attr in self.optimized_config.num_classes:
                baseline_acc = baseline_results['metrics'].get(f'{attr}_accuracy', 0)
                optimized_acc = optimized_results['metrics'].get(f'{attr}_accuracy', 0)
                
                # ç®€åŒ–çš„æ˜¾è‘—æ€§æ£€éªŒï¼ˆåŸºäºå‡†ç¡®ç‡å·®å¼‚ï¼‰
                diff = optimized_acc - baseline_acc
                
                # ä½¿ç”¨ç»éªŒæ€§çš„æ˜¾è‘—æ€§åˆ¤æ–­
                if abs(diff) > 0.05:  # 5%ä»¥ä¸Šå·®å¼‚è®¤ä¸ºæ˜¾è‘—
                    p_value = 0.01 if abs(diff) > 0.1 else 0.05
                    significant = True
                else:
                    p_value = 0.1
                    significant = False
                
                significance_results[attr] = {
                    'baseline_acc': baseline_acc,
                    'optimized_acc': optimized_acc,
                    'difference': diff,
                    'improvement_pct': (diff / max(baseline_acc, 0.001)) * 100,
                    'p_value': p_value,
                    'significant': significant
                }
        
        # æ•´ä½“æ˜¾è‘—æ€§
        baseline_mean = baseline_results['metrics'].get('mean_accuracy', 0)
        optimized_mean = optimized_results['metrics'].get('mean_accuracy', 0)
        overall_diff = optimized_mean - baseline_mean
        
        significance_results['overall'] = {
            'baseline_mean': baseline_mean,
            'optimized_mean': optimized_mean,
            'difference': overall_diff,
            'improvement_pct': (overall_diff / max(baseline_mean, 0.001)) * 100,
            'significant': abs(overall_diff) > 0.02
        }
        
        return significance_results
    
    def generate_report(self, baseline_results: Dict, optimized_results: Dict,
                       significance_results: Dict) -> str:
        """ç”Ÿæˆè¯¦ç»†å¯¹æ¯”æŠ¥å‘Š"""
        report_lines = []
        report_lines.append("# CelebA A/Bæµ‹è¯•å¯¹æ¯”æŠ¥å‘Š")
        report_lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")
        
        # æ•´ä½“å¯¹æ¯”
        report_lines.append("## ğŸ“Š æ•´ä½“æ€§èƒ½å¯¹æ¯”")
        overall = significance_results['overall']
        report_lines.append(f"| æŒ‡æ ‡ | Baseline | ä¼˜åŒ–ç‰ˆ | å·®å¼‚ | æ”¹è¿›% |")
        report_lines.append(f"|------|----------|--------|------|-------|")
        report_lines.append(f"| Mean Accuracy | {overall['baseline_mean']:.4f} | {overall['optimized_mean']:.4f} | {overall['difference']:+.4f} | {overall['improvement_pct']:+.2f}% |")
        report_lines.append(f"| å¹³å‡æŸå¤± | {baseline_results['avg_loss']:.4f} | {optimized_results['avg_loss']:.4f} | {optimized_results['avg_loss'] - baseline_results['avg_loss']:+.4f} | - |")
        
        # æ˜¾è‘—æ€§åˆ¤æ–­
        if overall['significant']:
            if overall['difference'] > 0:
                report_lines.append(f"\nğŸ‰ **æ•´ä½“ç»“è®º**: ä¼˜åŒ–ç‰ˆæ˜¾è‘—ä¼˜äºBaseline (+{overall['difference']:.3f})")
            else:
                report_lines.append(f"\nâš ï¸ **æ•´ä½“ç»“è®º**: ä¼˜åŒ–ç‰ˆæ˜¾è‘—åŠ£äºBaseline ({overall['difference']:.3f})")
        else:
            report_lines.append(f"\nğŸ” **æ•´ä½“ç»“è®º**: ä¸¤ç‰ˆæœ¬æ€§èƒ½æ— æ˜¾è‘—å·®å¼‚ ({overall['difference']:+.3f})")
        
        # å„å±æ€§ç»„è¯¦ç»†å¯¹æ¯”
        report_lines.append(f"\n## ğŸ¯ å„å±æ€§ç»„è¯¦ç»†å¯¹æ¯”")
        report_lines.append(f"| å±æ€§ç»„ | Baseline | ä¼˜åŒ–ç‰ˆ | å·®å¼‚ | æ”¹è¿›% | æ˜¾è‘—æ€§ |")
        report_lines.append(f"|--------|----------|--------|------|-------|--------|")
        
        # æŒ‰æ”¹è¿›å¹…åº¦æ’åº
        attr_results = [(attr, results) for attr, results in significance_results.items() 
                       if attr != 'overall']
        attr_results.sort(key=lambda x: x[1]['difference'], reverse=True)
        
        for attr, results in attr_results:
            significance_mark = "âœ…" if results['significant'] and results['difference'] > 0 else \
                              "âŒ" if results['significant'] and results['difference'] < 0 else "â–"
            
            report_lines.append(f"| {attr} | {results['baseline_acc']:.4f} | {results['optimized_acc']:.4f} | {results['difference']:+.4f} | {results['improvement_pct']:+.2f}% | {significance_mark} |")
        
        # æ€§èƒ½åˆ†æ
        report_lines.append(f"\n## ğŸ“ˆ æ€§èƒ½åˆ†æ")
        
        improvements = [r for _, r in attr_results if r['difference'] > 0.01]
        degradations = [r for _, r in attr_results if r['difference'] < -0.01]
        
        report_lines.append(f"- **æ˜¾è‘—æ”¹è¿›**: {len(improvements)} ä¸ªå±æ€§ç»„")
        report_lines.append(f"- **æ˜¾è‘—é€€åŒ–**: {len(degradations)} ä¸ªå±æ€§ç»„")
        report_lines.append(f"- **åŸºæœ¬æŒå¹³**: {len(attr_results) - len(improvements) - len(degradations)} ä¸ªå±æ€§ç»„")
        
        if improvements:
            best_improvement = max(improvements, key=lambda x: x['difference'])
            report_lines.append(f"- **æœ€å¤§æ”¹è¿›**: {best_improvement['difference']:.3f} (+{best_improvement['improvement_pct']:.1f}%)")
        
        if degradations:
            worst_degradation = min(degradations, key=lambda x: x['difference'])
            report_lines.append(f"- **æœ€å¤§é€€åŒ–**: {worst_degradation['difference']:.3f} ({worst_degradation['improvement_pct']:.1f}%)")
        
        # å»ºè®®
        report_lines.append(f"\n## ğŸ’¡ å»ºè®®")
        if overall['difference'] > 0.02:
            report_lines.append(f"âœ… **å»ºè®®é‡‡ç”¨ä¼˜åŒ–ç‰ˆæœ¬**ï¼Œæ•´ä½“æ€§èƒ½æå‡æ˜¾è‘—")
        elif overall['difference'] > 0:
            report_lines.append(f"ğŸ¤” **å¯è€ƒè™‘é‡‡ç”¨ä¼˜åŒ–ç‰ˆæœ¬**ï¼Œæœ‰è½»å¾®æ”¹è¿›")
        else:
            report_lines.append(f"âš ï¸ **å»ºè®®ç»§ç»­ä¼˜åŒ–**ï¼Œå½“å‰ç‰ˆæœ¬æœªè¾¾é¢„æœŸæ”¹è¿›")
        
        return "\n".join(report_lines)
    
    def run_ab_test(self) -> Dict:
        """è¿è¡Œå®Œæ•´A/Bæµ‹è¯•"""
        print("ğŸš€ å¼€å§‹CelebA A/Bæµ‹è¯•")
        print("=" * 60)
        
        # 1. åŠ è½½æ¨¡å‹
        baseline_model = self.load_model(self.baseline_checkpoint, self.baseline_config, "Baseline")
        optimized_model = self.load_model(self.optimized_checkpoint, self.optimized_config, "ä¼˜åŒ–")
        
        # 2. è®¾ç½®æ•°æ®
        baseline_dataloader, optimized_dataloader = self.setup_dataloaders()
        
        # 3. è¯„ä¼°æ¨¡å‹
        baseline_results = self.evaluate_model(baseline_model, baseline_dataloader, 
                                             self.baseline_config, "Baseline")
        optimized_results = self.evaluate_model(optimized_model, optimized_dataloader,
                                              self.optimized_config, "ä¼˜åŒ–")
        
        # 4. ç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ
        significance_results = self.compute_statistical_significance(baseline_results, optimized_results)
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report(baseline_results, optimized_results, significance_results)
        
        # 6. ä¿å­˜ç»“æœ
        results = {
            'baseline_results': baseline_results,
            'optimized_results': optimized_results,
            'significance_results': significance_results,
            'report': report,
            'test_info': {
                'baseline_checkpoint': self.baseline_checkpoint,
                'optimized_checkpoint': self.optimized_checkpoint,
                'data_path': self.data_path,
                'batch_size': self.batch_size,
                'device': str(self.device),
                'test_time': datetime.now().isoformat()
            }
        }
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='CelebA A/Bæµ‹è¯•')
    parser.add_argument('--baseline', type=str, required=True,
                       help='Baselineæ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--optimized', type=str, required=True,
                       help='ä¼˜åŒ–æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--data-path', type=str, default='D:\\KKK\\data\\CelebA',
                       help='CelebAæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--batch-size', type=int, default=32,
                       help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--output', type=str, default='ab_test_results',
                       help='è¾“å‡ºæ–‡ä»¶å‰ç¼€')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥
    if not os.path.exists(args.baseline):
        print(f"âŒ Baselineæ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {args.baseline}")
        return
    
    if not os.path.exists(args.optimized):
        print(f"âŒ ä¼˜åŒ–æ¨¡å‹æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {args.optimized}")
        return
    
    if not os.path.exists(args.data_path):
        print(f"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {args.data_path}")
        return
    
    # è¿è¡ŒA/Bæµ‹è¯•
    tester = ABTester(args.baseline, args.optimized, args.data_path, args.batch_size)
    
    try:
        results = tester.run_ab_test()
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜JSONç»“æœ
        json_path = f"{args.output}_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            # ç§»é™¤ä¸å¯åºåˆ—åŒ–çš„éƒ¨åˆ†
            serializable_results = {
                'significance_results': results['significance_results'],
                'test_info': results['test_info'],
                'baseline_metrics': results['baseline_results']['metrics'],
                'optimized_metrics': results['optimized_results']['metrics'],
                'baseline_loss': results['baseline_results']['avg_loss'],
                'optimized_loss': results['optimized_results']['avg_loss']
            }
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜MarkdownæŠ¥å‘Š
        report_path = f"{args.output}_{timestamp}.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(results['report'])
        
        print(f"\n" + "=" * 60)
        print(f"âœ… A/Bæµ‹è¯•å®Œæˆ!")
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_path}")
        print(f"ğŸ“Š æ•°æ®ç»“æœ: {json_path}")
        print(f"" + "=" * 60)
        
        # æ‰“å°æ ¸å¿ƒç»“è®º
        overall = results['significance_results']['overall']
        print(f"\nğŸ¯ æ ¸å¿ƒç»“è®º:")
        print(f"  Baseline Mean Accuracy: {overall['baseline_mean']:.4f}")
        print(f"  ä¼˜åŒ–ç‰ˆ Mean Accuracy: {overall['optimized_mean']:.4f}")
        print(f"  æ”¹è¿›å¹…åº¦: {overall['difference']:+.4f} ({overall['improvement_pct']:+.2f}%)")
        
        if overall['significant']:
            if overall['difference'] > 0:
                print(f"  ğŸ‰ ä¼˜åŒ–ç‰ˆæ˜¾è‘—ä¼˜äºBaseline!")
            else:
                print(f"  âš ï¸ ä¼˜åŒ–ç‰ˆæ˜¾è‘—åŠ£äºBaseline")
        else:
            print(f"  ğŸ” ä¸¤ç‰ˆæœ¬æ€§èƒ½æ— æ˜¾è‘—å·®å¼‚")
        
    except Exception as e:
        print(f"âŒ A/Bæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 