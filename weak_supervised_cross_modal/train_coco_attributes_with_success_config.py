"""
åŸºäºæˆåŠŸè®­ç»ƒç»éªŒçš„COCOAttributesè®­ç»ƒè„šæœ¬
ç»“åˆä¹‹å‰åœ¨cocottributes-masterä¸Šçš„æˆåŠŸé…ç½®
"""
import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import logging
import argparse
from tqdm import tqdm
import numpy as np
import time
from datetime import datetime
import joblib
from PIL import Image
import torchvision.transforms as transforms
from pycocotools.coco import COCO

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def setup_logging(log_dir):
    """è®¾ç½®æ—¥å¿—"""
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, f'training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    
    return log_file

def get_image_crop(img, x, y, width, height, crop_size=224, padding=16):
    """
    Get the image crop for the object specified in the COCO annotations.
    We crop in such a way that in the final resized image, there is `context padding` amount of image data around the object.
    This is the same as is used in RCNN to allow for additional image context.
    """
    # Scale used to compute the new bbox for the image such that there is surrounding context.
    scale = crop_size / (crop_size - padding * 2)

    # Calculate semi-width and semi-height
    semi_width = width / 2
    semi_height = height / 2

    # Calculate the center of the crop
    centerx = x + semi_width
    centery = y + semi_height

    img_width, img_height = img.size

    # We get the crop using the semi- height and width from the center of the crop.
    upper = max(0, centery - (semi_height * scale))
    lower = min(img_height, centery + (semi_height * scale))
    left = max(0, centerx - (semi_width * scale))
    right = min(img_width, centerx + (semi_width * scale))

    crop_img = img.crop((left, upper, right, lower))

    if 0 in crop_img.size:
        print(img.size)
        print("lowx {0}\nlowy {1}\nhighx {2}\nhighy {3}".format(
            left, upper, right, lower))

    return crop_img

class COCOAttributesDataset2017(Dataset):
    """åŸºäºæˆåŠŸç»éªŒçš„COCO Attributes 2017æ•°æ®é›†"""
    def __init__(self, attributes_file, annotations_file, dataset_root,
                 transforms=None, target_transforms=None,
                 split='train2014', train=True,
                 n_attrs=204, crop_size=224):
        self.attributes_dataset = joblib.load(attributes_file)
        self.coco = COCO(annotations_file)
        self.dataset_root = dataset_root

        self.transforms = transforms
        self.target_transforms = target_transforms

        self.split = split
        self.train = train
        self.n_attrs = n_attrs
        self.crop_size = crop_size

        # é€‚é…æ•°æ®æ ¼å¼
        logging.info("Loading attributes dataset")
        self.data = []
        
        # æ£€æŸ¥æ•°æ®é›†æ ¼å¼
        if 'ann_attrs' in self.attributes_dataset:
            # æ–°æ ¼å¼
            logging.info("Using new format attributes dataset with ann_attrs")
            # è·å–æ‰€æœ‰å±æ€§å‘é‡
            for ann_id, ann_attr in self.attributes_dataset['ann_attrs'].items():
                if ann_attr['split'] == split:
                    self.data.append(ann_id)
        elif 'ann_vecs' in self.attributes_dataset:
            # æ—§æ ¼å¼
            logging.info("Using old format attributes dataset with ann_vecs")
            # è·å–æ‰€æœ‰å±æ€§å‘é‡
            for patch_id, _ in self.attributes_dataset['ann_vecs'].items():
                if self.attributes_dataset['split'][patch_id] == split:
                    self.data.append(patch_id)
        else:
            raise ValueError("Unknown attributes dataset format")

        # å±æ€§åç§°åˆ—è¡¨
        if 'attributes' in self.attributes_dataset:
            self.attributes = sorted(
                self.attributes_dataset['attributes'], key=lambda x: x['id'])
        else:
            # å¦‚æœæ²¡æœ‰å±æ€§åç§°åˆ—è¡¨ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„
            self.attributes = [{'id': i} for i in range(n_attrs)]

        logging.info(f"Loaded {len(self.data)} samples for {split}")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        ann_id = self.data[index]

        # é€‚é…æ–°çš„æ•°æ®æ ¼å¼
        if 'ann_vecs' in self.attributes_dataset:
            # æ—§æ ¼å¼
            attrs = self.attributes_dataset['ann_vecs'][ann_id]
            patch_id_to_ann_id = self.attributes_dataset['patch_id_to_ann_id']
            ann_id_actual = patch_id_to_ann_id[ann_id]
        else:
            # æ–°æ ¼å¼
            attrs = self.attributes_dataset['ann_attrs'][ann_id]['attrs_vector']
            ann_id_actual = int(ann_id)

        attrs = (attrs > 0).astype(np.float64)

        # coco.loadAnns returns a list
        try:
            ann = self.coco.loadAnns(ann_id_actual)[0]
            image = self.coco.loadImgs(ann['image_id'])[0]
        except:
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªéšæœºæ ·æœ¬
            logging.warning(f"Failed to load annotation {ann_id_actual}, using random sample")
            return self.__getitem__((index + 1) % len(self.data))

        x, y, width, height = ann["bbox"]

        # ä¿®æ”¹ä¸ºä½¿ç”¨2017ç‰ˆæœ¬çš„è·¯å¾„
        split_dir = 'train2017' if self.split == 'train2014' else 'val2017'
        
        try:
            img = Image.open(os.path.join(self.dataset_root, split_dir,
                                    image["file_name"])).convert('RGB')
        except:
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªéšæœºæ ·æœ¬
            logging.warning(f"Failed to load image {image['file_name']}, using random sample")
            return self.__getitem__((index + 1) % len(self.data))

        # Crop out the object with context padding.
        img = get_image_crop(img, x, y, width, height, self.crop_size)

        if self.transforms is not None:
            img = self.transforms(img)

        if self.target_transforms is not None:
            attrs = self.target_transforms(attrs)

        # è½¬æ¢ä¸ºæœ¬é¡¹ç›®æœŸæœ›çš„æ ¼å¼
        attrs_tensor = torch.tensor(attrs, dtype=torch.float32)
        
        # å°†å±æ€§åˆ†ç»„ä¸ºä¸åŒç±»åˆ«ï¼ˆåŸºäºCOCOAttributesçš„204ä¸ªå±æ€§ï¼‰
        targets = {
            'color': attrs_tensor[:12],      # å‰12ä¸ªå±æ€§ä½œä¸ºé¢œè‰²
            'material': attrs_tensor[12:27], # æ¥ä¸‹æ¥15ä¸ªå±æ€§ä½œä¸ºæè´¨
            'shape': attrs_tensor[27:47],    # æ¥ä¸‹æ¥20ä¸ªå±æ€§ä½œä¸ºå½¢çŠ¶
            'texture': attrs_tensor[47:57],  # æ¥ä¸‹æ¥10ä¸ªå±æ€§ä½œä¸ºçº¹ç†
            'size': attrs_tensor[57:62],     # æ¥ä¸‹æ¥5ä¸ªå±æ€§ä½œä¸ºå¤§å°
            'other': attrs_tensor[62:70]     # æ¥ä¸‹æ¥8ä¸ªå±æ€§ä½œä¸ºå…¶ä»–
        }

        return img, targets

def train_epoch(model, dataloader, criterion, optimizer, device, epoch, config):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    num_batches = 0
    
    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}')
    
    for batch_idx, (images, targets) in enumerate(progress_bar):
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        images = images.to(device)
        targets = {k: v.to(device) for k, v in targets.items()}
        
        # æ¸…é›¶æ¢¯åº¦
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        outputs = model(images)
        
        # è®¡ç®—æŸå¤±
        loss, loss_dict = criterion(outputs, targets, epoch)
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆ
        if torch.isnan(loss) or torch.isinf(loss):
            logging.warning(f"è·³è¿‡æ— æ•ˆæŸå¤±: {loss.item()}")
            continue
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # æ›´æ–°å‚æ•°
        optimizer.step()
        
        # ç´¯è®¡æŸå¤±
        total_loss += loss.item()
        num_batches += 1
        
        # æ›´æ–°è¿›åº¦æ¡
        avg_loss = total_loss / num_batches
        progress_bar.set_postfix({
            'loss': f'{avg_loss:.4f}',
            'lr': f'{optimizer.param_groups[0]["lr"]:.6f}'
        })
        
        # æ¯100ä¸ªæ‰¹æ¬¡è®°å½•ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
        if batch_idx % 100 == 0:
            logging.info(f'Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}')
            for key, value in loss_dict.items():
                if isinstance(value, torch.Tensor):
                    logging.info(f'  {key}: {value.item():.4f}')
    
    return total_loss / max(1, num_batches)

def validate_epoch(model, dataloader, criterion, device, epoch):
    """éªŒè¯ä¸€ä¸ªepoch"""
    model.eval()
    total_loss = 0.0
    num_batches = 0
    
    with torch.no_grad():
        progress_bar = tqdm(dataloader, desc=f'Validation')
        
        for batch_idx, (images, targets) in enumerate(progress_bar):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            images = images.to(device)
            targets = {k: v.to(device) for k, v in targets.items()}
            
            # å‰å‘ä¼ æ’­
            outputs = model(images)
            
            # è®¡ç®—æŸå¤±
            loss, loss_dict = criterion(outputs, targets, epoch)
            
            if not torch.isnan(loss) and not torch.isinf(loss):
                total_loss += loss.item()
                num_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            if num_batches > 0:
                avg_loss = total_loss / num_batches
                progress_bar.set_postfix({'val_loss': f'{avg_loss:.4f}'})
    
    return total_loss / max(1, num_batches)

def save_checkpoint(model, optimizer, scheduler, epoch, train_loss, val_loss, config, save_path):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'train_loss': train_loss,
        'val_loss': val_loss,
        'config': config,
        'timestamp': datetime.now().isoformat()
    }
    
    torch.save(checkpoint, save_path)
    logging.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {save_path}")

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    parser = argparse.ArgumentParser(description='åŸºäºæˆåŠŸç»éªŒçš„COCOAttributesè®­ç»ƒ')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1e-4, help='åˆå§‹å­¦ä¹ ç‡')
    parser.add_argument('--batch_size', type=int, default=16, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--save_dir', type=str, default='./checkpoints_coco_success', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--log_dir', type=str, default='./logs_coco_success', help='æ—¥å¿—ä¿å­˜ç›®å½•')
    parser.add_argument('--resume', type=str, default=None, help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--eval_only', action='store_true', help='ä»…è¿›è¡Œè¯„ä¼°')
    parser.add_argument('--coco_dataset_root', type=str, default='D:/KKK/COCO_Dataset', help='COCOæ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--attributes_file', type=str, 
                       default='D:/KKK/jieoulunwen/data/cocottributes-master/cocottributes-master/MSCOCO/cocottributes_new_version.jbl',
                       help='å±æ€§æ•°æ®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    log_file = setup_logging(args.log_dir)
    logging.info("å¼€å§‹åŸºäºæˆåŠŸç»éªŒçš„COCOAttributesè®­ç»ƒ...")
    logging.info(f"å‚æ•°: {args}")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logging.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        # æ•°æ®å˜æ¢
        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                       std=[0.229, 0.224, 0.225])
        train_transforms = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            normalize,
        ])
        val_transforms = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            normalize,
        ])

        # åˆ›å»ºæ•°æ®é›†
        logging.info("åˆ›å»ºæ•°æ®é›†...")
        train_dataset = COCOAttributesDataset2017(
            args.attributes_file,
            f"{args.coco_dataset_root}/annotations/instances_train2017.json",
            args.coco_dataset_root,
            transforms=train_transforms,
            split='train2014',
            train=True
        )
        
        val_dataset = COCOAttributesDataset2017(
            args.attributes_file,
            f"{args.coco_dataset_root}/annotations/instances_val2017.json",
            args.coco_dataset_root,
            transforms=val_transforms,
            split='val2014',
            train=False
        )

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=args.batch_size,
            shuffle=True,
            num_workers=0,  # åœ¨Windowsä¸Šè®¾ä¸º0é¿å…å¤šè¿›ç¨‹é—®é¢˜
            pin_memory=False  # CPUè®­ç»ƒæ—¶è®¾ä¸ºFalse
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=0,
            pin_memory=False
        )

        logging.info(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        logging.info(f"  è®­ç»ƒé›†: {len(train_dataset)} ä¸ªæ ·æœ¬")
        logging.info(f"  éªŒè¯é›†: {len(val_dataset)} ä¸ªæ ·æœ¬")
        
        # å¯¼å…¥é…ç½®å’Œæ¨¡å‹
        from config.base_config import get_config
        config = get_config('COCOAttributes')
        
        # è¦†ç›–é…ç½®å‚æ•°
        config.learning_rate = args.lr
        config.batch_size = args.batch_size
        
        logging.info(f"é…ç½®åŠ è½½æˆåŠŸ: {config.dataset_name}")
        
        # å¯¼å…¥æ¨¡å‹
        from models import WeakSupervisedCrossModalAlignment
        model = WeakSupervisedCrossModalAlignment(config).to(device)
        logging.info("æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        logging.info(f"æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")
        
        # å¯¼å…¥æŸå¤±å‡½æ•°
        from training.losses import ComprehensiveLoss
        criterion = ComprehensiveLoss(config)
        logging.info("æŸå¤±å‡½æ•°åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ï¼ˆåŸºäºæˆåŠŸç»éªŒï¼‰
        optimizer = optim.Adam(
            model.parameters(), 
            lr=config.learning_rate,
            weight_decay=1e-4,
            betas=(0.9, 0.999)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = optim.lr_scheduler.StepLR(
            optimizer,
            step_size=10,
            gamma=0.1
        )
        
        logging.info("ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(args.save_dir, exist_ok=True)
        
        # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœæŒ‡å®šï¼‰
        start_epoch = 0
        best_val_loss = float('inf')
        
        if args.resume and os.path.exists(args.resume):
            logging.info(f"æ¢å¤è®­ç»ƒ: {args.resume}")
            checkpoint = torch.load(args.resume, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            if checkpoint.get('scheduler_state_dict'):
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            start_epoch = checkpoint['epoch'] + 1
            best_val_loss = checkpoint.get('val_loss', float('inf'))
            logging.info(f"ä»epoch {start_epoch}æ¢å¤è®­ç»ƒ")
        
        # ä»…è¯„ä¼°æ¨¡å¼
        if args.eval_only:
            logging.info("ä»…è¿›è¡Œæ¨¡å‹è¯„ä¼°...")
            val_loss = validate_epoch(model, val_loader, criterion, device, 0)
            logging.info(f"éªŒè¯æŸå¤±: {val_loss:.4f}")
            return
        
        # è®­ç»ƒå¾ªç¯
        logging.info(f"å¼€å§‹è®­ç»ƒï¼Œå…± {args.epochs} ä¸ªepoch")
        
        for epoch in range(start_epoch, args.epochs):
            epoch_start_time = time.time()
            
            logging.info(f"\n=== Epoch {epoch + 1}/{args.epochs} ===")
            
            # è®­ç»ƒ
            train_loss = train_epoch(
                model, train_loader, criterion, optimizer, device, epoch, config
            )
            
            # éªŒè¯
            val_loss = validate_epoch(
                model, val_loader, criterion, device, epoch
            )
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            current_lr = scheduler.get_last_lr()[0]
            
            epoch_time = time.time() - epoch_start_time
            
            logging.info(f"Epoch {epoch + 1} å®Œæˆ:")
            logging.info(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            logging.info(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
            logging.info(f"  å­¦ä¹ ç‡: {current_lr:.6f}")
            logging.info(f"  è€—æ—¶: {epoch_time:.2f}ç§’")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_model_path = os.path.join(args.save_dir, 'best_coco_attributes_success.pth')
                save_checkpoint(model, optimizer, scheduler, epoch, train_loss, val_loss, config, best_model_path)
                logging.info(f"æ–°çš„æœ€ä½³æ¨¡å‹! éªŒè¯æŸå¤±: {val_loss:.4f}")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % 10 == 0:
                checkpoint_path = os.path.join(args.save_dir, f'checkpoint_epoch_{epoch+1}.pth')
                save_checkpoint(model, optimizer, scheduler, epoch, train_loss, val_loss, config, checkpoint_path)
        
        logging.info(f"\nè®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        logging.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {args.save_dir}")
        logging.info(f"æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")
        
        return True
        
    except Exception as e:
        logging.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\nğŸ‰ åŸºäºæˆåŠŸç»éªŒçš„COCOAttributesè®­ç»ƒå®Œæˆ!")
    else:
        print("\nğŸ’¥ è®­ç»ƒå¤±è´¥!") 