#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çœŸå®CUBæ•°æ®é›†è·¨æ¨¡æ€è®­ç»ƒå™¨
çœŸæ­£ä½¿ç”¨å›¾åƒæ•°æ®è¿›è¡Œè·¨æ¨¡æ€å±æ€§å¯¹é½è®­ç»ƒ
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision.models as models
import torchvision.transforms as transforms
import numpy as np
from pathlib import Path
from PIL import Image
from tqdm import tqdm
import argparse
import logging
import json
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from train_cub import CUBDataset
from utils.config_loader import ConfigLoader
from utils.decorators import error_handler, performance_monitor

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RealCrossModalModel(nn.Module):
    """çœŸå®çš„è·¨æ¨¡æ€æ¨¡å‹ - ä½¿ç”¨çœŸå®å›¾åƒç‰¹å¾"""
    
    def __init__(self, num_classes=200, num_attributes=312, hidden_dim=512, dropout=0.1):
        super().__init__()
        
        # çœŸå®çš„è§†è§‰ç¼–ç å™¨ - ä½¿ç”¨é¢„è®­ç»ƒResNet
        self.visual_backbone = models.resnet50(pretrained=True)
        # ç§»é™¤æœ€åçš„åˆ†ç±»å±‚
        self.visual_backbone = nn.Sequential(*list(self.visual_backbone.children())[:-1])
        
        # å†»ç»“éƒ¨åˆ†é¢„è®­ç»ƒå‚æ•°
        for param in self.visual_backbone.parameters():
            param.requires_grad = False
        
        # è§£å†»æœ€åå‡ å±‚
        for param in self.visual_backbone[-2:].parameters():
            param.requires_grad = True
        
        # è§†è§‰ç‰¹å¾æŠ•å½±
        self.visual_projector = nn.Sequential(
            nn.Linear(2048, hidden_dim),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim),
            nn.Dropout(dropout)
        )
        
        # å±æ€§ç¼–ç å™¨
        self.attribute_encoder = nn.Sequential(
            nn.Linear(num_attributes, hidden_dim),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim),
            nn.Dropout(dropout * 0.5),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim)
        )
        
        # è·¨æ¨¡æ€èåˆ
        self.fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.Dropout(dropout * 0.5)
        )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Linear(hidden_dim // 2, num_classes)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
        
    def forward(self, images, attributes):
        """å‰å‘ä¼ æ’­ - çœŸå®ä½¿ç”¨å›¾åƒæ•°æ®"""
        batch_size = images.size(0)
        
        # çœŸå®çš„è§†è§‰ç‰¹å¾æå–
        with torch.no_grad():
            visual_features = self.visual_backbone(images)  # ä½¿ç”¨çœŸå®å›¾åƒï¼
        visual_features = visual_features.view(batch_size, -1)
        visual_features = self.visual_projector(visual_features)
        
        # å±æ€§ç‰¹å¾
        attribute_features = self.attribute_encoder(attributes)
        
        # è·¨æ¨¡æ€èåˆ
        combined_features = torch.cat([visual_features, attribute_features], dim=1)
        
        # æ³¨æ„åŠ›æƒé‡
        attention_weights = self.attention(combined_features)
        
        # åŠ æƒèåˆ
        weighted_visual = visual_features * attention_weights
        weighted_attribute = attribute_features * (1 - attention_weights)
        final_features = torch.cat([weighted_visual, weighted_attribute], dim=1)
        
        # æœ€ç»ˆèåˆå’Œåˆ†ç±»
        fused_features = self.fusion(final_features)
        logits = self.classifier(fused_features)
        
        return {
            'predictions': {'species': logits},
            'logits': logits,
            'visual_features': visual_features,
            'attribute_features': attribute_features,
            'attention_weights': attention_weights
        }

class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    
    def __init__(self, patience=15, min_delta=0.001, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_score = None
        self.counter = 0
        self.best_weights = None
        self.early_stop = False
        
    def __call__(self, val_score, model):
        if self.best_score is None:
            self.best_score = val_score
            self.save_checkpoint(model)
        elif val_score < self.best_score + self.min_delta:
            self.counter += 1
            logger.info(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
                if self.restore_best_weights:
                    model.load_state_dict(self.best_weights)
                    logger.info('Restored best model weights')
        else:
            self.best_score = val_score
            self.save_checkpoint(model)
            self.counter = 0
            
    def save_checkpoint(self, model):
        """ä¿å­˜æœ€ä½³æ¨¡å‹æƒé‡"""
        self.best_weights = model.state_dict().copy()

class RealCUBTrainer:
    """çœŸå®CUBæ•°æ®è®­ç»ƒå™¨"""
    
    def __init__(self, config_path: str):
        self.config = ConfigLoader(config_path)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(42)
        np.random.seed(42)
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.checkpoint_dir = Path('checkpoints')
        self.checkpoint_dir.mkdir(exist_ok=True)
        
        # è®­ç»ƒå†å²è®°å½•
        self.history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'learning_rates': []
        }
        
        self._setup_data()
        self._setup_model()
        self._setup_training()
        self._setup_early_stopping()
        
    def _setup_data(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        # æ•°æ®å˜æ¢ - é€‚åˆResNetçš„é¢„å¤„ç†
        train_transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=15),
            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        test_transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # æ•°æ®é›†
        data_root = self.config.get('data.root_path', '../data/CUB_200_2011/CUB_200_2011')
        
        self.train_dataset = CUBDataset(
            data_root=data_root,
            split='train',
            transform=train_transform,
            use_attributes=True
        )
        
        self.test_dataset = CUBDataset(
            data_root=data_root,
            split='test', 
            transform=test_transform,
            use_attributes=True
        )
        
        # æ•°æ®åŠ è½½å™¨
        batch_size = self.config.get('data.batch_size', 16)  # å‡å°batch sizeä»¥é€‚åº”çœŸå®å›¾åƒå¤„ç†
        num_workers = self.config.get('data.num_workers', 4)
        
        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True,
            drop_last=True
        )
        
        self.test_loader = DataLoader(
            self.test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True
        )
        
        logger.info(f"è®­ç»ƒé›†: {len(self.train_dataset)} æ ·æœ¬")
        logger.info(f"æµ‹è¯•é›†: {len(self.test_dataset)} æ ·æœ¬")
        
    def _setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        dropout = self.config.get('model.dropout', 0.1)
        
        self.model = RealCrossModalModel(
            num_classes=200,
            num_attributes=312,
            hidden_dim=512,
            dropout=dropout
        ).to(self.device)
        
        # è®¡ç®—å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        logger.info(f"æ¨¡å‹æ€»å‚æ•°æ•°é‡: {total_params:,}")
        logger.info(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
        
    def _setup_training(self):
        """è®¾ç½®è®­ç»ƒç»„ä»¶"""
        self.criterion = nn.CrossEntropyLoss()
        
        # ä¼˜åŒ–å™¨ - ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡
        lr = self.config.get('training.learning_rate', 1e-4)
        weight_decay = self.config.get('training.weight_decay', 1e-4)
        
        # åˆ†ç»„å‚æ•° - é¢„è®­ç»ƒéƒ¨åˆ†ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
        backbone_params = []
        other_params = []
        
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                if 'visual_backbone' in name:
                    backbone_params.append(param)
                else:
                    other_params.append(param)
        
        self.optimizer = optim.Adam([
            {'params': backbone_params, 'lr': lr * 0.1},  # é¢„è®­ç»ƒéƒ¨åˆ†ä½¿ç”¨æ›´å°å­¦ä¹ ç‡
            {'params': other_params, 'lr': lr}
        ], weight_decay=weight_decay)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler_type = self.config.get('training.scheduler_type', 'cosine')
        
        if scheduler_type == 'cosine':
            T_max = self.config.get('training.num_epochs', 50)
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer, T_max=T_max
            )
        else:
            step_size = self.config.get('training.step_size', 15)
            gamma = self.config.get('training.gamma', 0.1)
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer, step_size=step_size, gamma=gamma
            )
            
        logger.info(f"ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨: {scheduler_type}")
        
    def _setup_early_stopping(self):
        """è®¾ç½®æ—©åœæœºåˆ¶"""
        patience = self.config.get('training.early_stopping.patience', 10)
        min_delta = self.config.get('training.early_stopping.min_delta', 0.001)
        
        self.early_stopping = EarlyStopping(
            patience=patience,
            min_delta=min_delta,
            restore_best_weights=True
        )
        
        logger.info(f"æ—©åœæœºåˆ¶: patience={patience}, min_delta={min_delta}")
    
    @error_handler(default_return=None)
    @performance_monitor()
    def train_epoch(self, epoch: int):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}')
        
        for batch_idx, batch in enumerate(pbar):
            images = batch['image'].to(self.device)
            class_ids = batch['class_id'].to(self.device)
            attributes = batch['attributes'].to(self.device)
            
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­ - ä½¿ç”¨çœŸå®å›¾åƒ
            outputs = self.model(images, attributes)
            loss = self.criterion(outputs['logits'], class_ids)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = torch.max(outputs['logits'].data, 1)
            total += class_ids.size(0)
            correct += (predicted == class_ids).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*correct/total:.2f}%',
                'LR': f'{self.optimizer.param_groups[0]["lr"]:.6f}'
            })
        
        epoch_loss = total_loss / len(self.train_loader)
        epoch_acc = 100. * correct / total
        
        return {
            'loss': epoch_loss,
            'accuracy': epoch_acc
        }
    
    @error_handler(default_return={})
    @performance_monitor()
    def evaluate(self):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch in tqdm(self.test_loader, desc='Evaluating'):
                images = batch['image'].to(self.device)
                class_ids = batch['class_id'].to(self.device)
                attributes = batch['attributes'].to(self.device)
                
                outputs = self.model(images, attributes)
                loss = self.criterion(outputs['logits'], class_ids)
                
                total_loss += loss.item()
                _, predicted = torch.max(outputs['logits'].data, 1)
                total += class_ids.size(0)
                correct += (predicted == class_ids).sum().item()
        
        return {
            'loss': total_loss / len(self.test_loader),
            'accuracy': 100. * correct / total
        }
    
    def save_model(self, filename: str, epoch: int, is_best: bool = False):
        """ä¿å­˜æ¨¡å‹"""
        save_path = self.checkpoint_dir / filename
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'history': self.history,
            'config': dict(self.config.config),
            'is_best': is_best
        }
        
        torch.save(checkpoint, save_path)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    def save_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_path = self.checkpoint_dir / 'real_training_history.json'
        with open(history_path, 'w') as f:
            json.dump(self.history, f, indent=2)
    
    def train(self, num_epochs: int = 50):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        logger.info(f"å¼€å§‹çœŸå®å›¾åƒè·¨æ¨¡æ€è®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepochs")
        logger.info(f"è®¾å¤‡: {self.device}")
        logger.info(f"ä½¿ç”¨çœŸå®ResNet50ç‰¹å¾æå–å™¨")
        
        best_acc = 0.0
        start_time = time.time()
        
        for epoch in range(num_epochs):
            epoch_start_time = time.time()
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch(epoch)
            
            # è¯„ä¼°
            val_metrics = self.evaluate()
            
            # è®°å½•å†å²
            current_lr = self.optimizer.param_groups[0]['lr']
            self.history['train_loss'].append(train_metrics['loss'])
            self.history['train_acc'].append(train_metrics['accuracy'])
            self.history['val_loss'].append(val_metrics['loss'])
            self.history['val_acc'].append(val_metrics['accuracy'])
            self.history['learning_rates'].append(current_lr)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            is_best = val_metrics['accuracy'] > best_acc
            if is_best:
                best_acc = val_metrics['accuracy']
                self.save_model('best_real_cub_model.pth', epoch, is_best=True)
            
            # å®šæœŸä¿å­˜æ¨¡å‹
            if (epoch + 1) % 10 == 0:
                self.save_model(f'real_checkpoint_epoch_{epoch+1}.pth', epoch)
            
            # æ—©åœæ£€æŸ¥
            self.early_stopping(val_metrics['accuracy'], self.model)
            
            # è®¡ç®—epochæ—¶é—´
            epoch_time = time.time() - epoch_start_time
            
            # æ‰“å°ç»“æœ
            logger.info(
                f"Epoch {epoch+1}/{num_epochs} ({epoch_time:.1f}s) - "
                f"Train Loss: {train_metrics['loss']:.4f}, "
                f"Train Acc: {train_metrics['accuracy']:.2f}% - "
                f"Val Loss: {val_metrics['loss']:.4f}, "
                f"Val Acc: {val_metrics['accuracy']:.2f}% - "
                f"LR: {current_lr:.6f} - "
                f"Best: {best_acc:.2f}%"
            )
            
            # ä¿å­˜è®­ç»ƒå†å²
            if (epoch + 1) % 5 == 0:
                self.save_history()
            
            # æ—©åœæ£€æŸ¥
            if self.early_stopping.early_stop:
                logger.info(f"Early stopping triggered at epoch {epoch+1}")
                break
        
        # ä¿å­˜æœ€åä¸€ä¸ªæ¨¡å‹
        self.save_model('last_real_cub_model.pth', epoch)
        self.save_history()
        
        # è®¡ç®—æ€»è®­ç»ƒæ—¶é—´
        total_time = time.time() - start_time
        logger.info(f"è®­ç»ƒå®Œæˆï¼æ€»æ—¶é—´: {total_time/3600:.2f}å°æ—¶")
        logger.info(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.2f}%")
        
        return best_acc

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='çœŸå®CUBæ•°æ®é›†è·¨æ¨¡æ€è®­ç»ƒ')
    parser.add_argument('--config', type=str, default='configs/cub_config.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = RealCUBTrainer(args.config)
    
    # å¼€å§‹è®­ç»ƒ
    best_acc = trainer.train(num_epochs=args.epochs)
    
    print(f"\nğŸ‰ çœŸå®è·¨æ¨¡æ€è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.2f}%")

if __name__ == '__main__':
    main() 