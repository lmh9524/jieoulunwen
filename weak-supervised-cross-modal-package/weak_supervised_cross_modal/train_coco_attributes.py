"""
COCOAttributesæ•°æ®é›†å®Œæ•´è®­ç»ƒè„šæœ¬
"""
import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import logging
import argparse
from tqdm import tqdm
import numpy as np
import time
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def setup_logging(log_dir):
    """è®¾ç½®æ—¥å¿—"""
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, f'training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    
    return log_file

def train_epoch(model, dataloader, criterion, optimizer, device, epoch, config):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    num_batches = 0
    
    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}')
    
    for batch_idx, (images, targets) in enumerate(progress_bar):
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        images = images.to(device)
        targets = {k: v.to(device) for k, v in targets.items()}
        
        # æ¸…é›¶æ¢¯åº¦
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        outputs = model(images)
        
        # è®¡ç®—æŸå¤±
        loss, loss_dict = criterion(outputs, targets, epoch)
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆ
        if torch.isnan(loss) or torch.isinf(loss):
            logging.warning(f"è·³è¿‡æ— æ•ˆæŸå¤±: {loss.item()}")
            continue
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # æ›´æ–°å‚æ•°
        optimizer.step()
        
        # ç´¯è®¡æŸå¤±
        total_loss += loss.item()
        num_batches += 1
        
        # æ›´æ–°è¿›åº¦æ¡
        avg_loss = total_loss / num_batches
        progress_bar.set_postfix({
            'loss': f'{avg_loss:.4f}',
            'lr': f'{optimizer.param_groups[0]["lr"]:.6f}'
        })
        
        # æ¯100ä¸ªæ‰¹æ¬¡è®°å½•ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
        if batch_idx % 100 == 0:
            logging.info(f'Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}')
            for key, value in loss_dict.items():
                if isinstance(value, torch.Tensor):
                    logging.info(f'  {key}: {value.item():.4f}')
    
    return total_loss / max(1, num_batches)

def validate_epoch(model, dataloader, criterion, device, epoch):
    """éªŒè¯ä¸€ä¸ªepoch"""
    model.eval()
    total_loss = 0.0
    num_batches = 0
    
    with torch.no_grad():
        progress_bar = tqdm(dataloader, desc=f'Validation')
        
        for batch_idx, (images, targets) in enumerate(progress_bar):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            images = images.to(device)
            targets = {k: v.to(device) for k, v in targets.items()}
            
            # å‰å‘ä¼ æ’­
            outputs = model(images)
            
            # è®¡ç®—æŸå¤±
            loss, loss_dict = criterion(outputs, targets, epoch)
            
            if not torch.isnan(loss) and not torch.isinf(loss):
                total_loss += loss.item()
                num_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            if num_batches > 0:
                avg_loss = total_loss / num_batches
                progress_bar.set_postfix({'val_loss': f'{avg_loss:.4f}'})
    
    return total_loss / max(1, num_batches)

def save_checkpoint(model, optimizer, scheduler, epoch, train_loss, val_loss, config, save_path):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'train_loss': train_loss,
        'val_loss': val_loss,
        'config': config,
        'timestamp': datetime.now().isoformat()
    }
    
    torch.save(checkpoint, save_path)
    logging.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {save_path}")

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    parser = argparse.ArgumentParser(description='COCOAttributesæ•°æ®é›†å®Œæ•´è®­ç»ƒ')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1e-4, help='åˆå§‹å­¦ä¹ ç‡')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--save_dir', type=str, default='./checkpoints_coco', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--log_dir', type=str, default='./logs_coco', help='æ—¥å¿—ä¿å­˜ç›®å½•')
    parser.add_argument('--resume', type=str, default=None, help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--eval_only', action='store_true', help='ä»…è¿›è¡Œè¯„ä¼°')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    log_file = setup_logging(args.log_dir)
    logging.info("å¼€å§‹COCOAttributesæ•°æ®é›†å®Œæ•´è®­ç»ƒ...")
    logging.info(f"å‚æ•°: {args}")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logging.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        # å¯¼å…¥é…ç½®
        from config.base_config import get_config
        config = get_config('COCOAttributes')
        
        # è¦†ç›–é…ç½®å‚æ•°
        config.learning_rate = args.lr
        config.batch_size = args.batch_size
        
        logging.info(f"é…ç½®åŠ è½½æˆåŠŸ: {config.dataset_name}")
        logging.info(f"å±æ€§æ•°é‡: {config.num_attributes}")
        logging.info(f"å±æ€§ç±»åˆ«: {config.num_classes}")
        
        # å¯¼å…¥æ•°æ®é€‚é…å™¨
        from data.dataset_adapters import COCOAttributesDatasetAdapter
        adapter = COCOAttributesDatasetAdapter(config)
        dataloaders = adapter.get_dataloaders()
        
        logging.info("æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        for split, dataloader in dataloaders.items():
            logging.info(f"  {split}: {len(dataloader.dataset)} ä¸ªæ ·æœ¬")
        
        # å¯¼å…¥æ¨¡å‹
        from models import WeakSupervisedCrossModalAlignment
        model = WeakSupervisedCrossModalAlignment(config).to(device)
        logging.info("æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        logging.info(f"æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")
        
        # å¯¼å…¥æŸå¤±å‡½æ•°
        from training.losses import ComprehensiveLoss
        criterion = ComprehensiveLoss(config)
        logging.info("æŸå¤±å‡½æ•°åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer = optim.AdamW(
            model.parameters(), 
            lr=config.learning_rate,
            weight_decay=1e-4,
            betas=(0.9, 0.999)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, 
            T_0=10,  # æ¯10ä¸ªepoché‡å¯ä¸€æ¬¡
            T_mult=2,  # é‡å¯å‘¨æœŸå€å¢
            eta_min=1e-6
        )
        
        logging.info("ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(args.save_dir, exist_ok=True)
        
        # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœæŒ‡å®šï¼‰
        start_epoch = 0
        best_val_loss = float('inf')
        
        if args.resume and os.path.exists(args.resume):
            logging.info(f"æ¢å¤è®­ç»ƒ: {args.resume}")
            checkpoint = torch.load(args.resume, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            if checkpoint.get('scheduler_state_dict'):
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            start_epoch = checkpoint['epoch'] + 1
            best_val_loss = checkpoint.get('val_loss', float('inf'))
            logging.info(f"ä»epoch {start_epoch}æ¢å¤è®­ç»ƒ")
        
        # ä»…è¯„ä¼°æ¨¡å¼
        if args.eval_only:
            logging.info("ä»…è¿›è¡Œæ¨¡å‹è¯„ä¼°...")
            val_loss = validate_epoch(model, dataloaders['val'], criterion, device, 0)
            logging.info(f"éªŒè¯æŸå¤±: {val_loss:.4f}")
            return
        
        # è®­ç»ƒå¾ªç¯
        logging.info(f"å¼€å§‹è®­ç»ƒï¼Œå…± {args.epochs} ä¸ªepoch")
        
        for epoch in range(start_epoch, args.epochs):
            epoch_start_time = time.time()
            
            logging.info(f"\n=== Epoch {epoch + 1}/{args.epochs} ===")
            
            # è®­ç»ƒ
            train_loss = train_epoch(
                model, dataloaders['train'], criterion, optimizer, device, epoch, config
            )
            
            # éªŒè¯
            val_loss = validate_epoch(
                model, dataloaders['val'], criterion, device, epoch
            )
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            current_lr = scheduler.get_last_lr()[0]
            
            epoch_time = time.time() - epoch_start_time
            
            logging.info(f"Epoch {epoch + 1} å®Œæˆ:")
            logging.info(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            logging.info(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
            logging.info(f"  å­¦ä¹ ç‡: {current_lr:.6f}")
            logging.info(f"  è€—æ—¶: {epoch_time:.2f}ç§’")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_model_path = os.path.join(args.save_dir, 'best_model.pth')
                save_checkpoint(model, optimizer, scheduler, epoch, train_loss, val_loss, config, best_model_path)
                logging.info(f"æ–°çš„æœ€ä½³æ¨¡å‹! éªŒè¯æŸå¤±: {val_loss:.4f}")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % 10 == 0:
                checkpoint_path = os.path.join(args.save_dir, f'checkpoint_epoch_{epoch+1}.pth')
                save_checkpoint(model, optimizer, scheduler, epoch, train_loss, val_loss, config, checkpoint_path)
        
        logging.info(f"\nè®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        logging.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {args.save_dir}")
        logging.info(f"æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")
        
        return True
        
    except Exception as e:
        logging.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\nğŸ‰ COCOAttributesè®­ç»ƒè„šæœ¬è¿è¡ŒæˆåŠŸ!")
    else:
        print("\nğŸ’¥ COCOAttributesè®­ç»ƒè„šæœ¬è¿è¡Œå¤±è´¥!")
