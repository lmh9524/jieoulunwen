"""
åŸºäºæˆåŠŸè®­ç»ƒç»éªŒçš„COCOAttributesè®­ç»ƒè„šæœ¬
ç»“åˆä¹‹å‰åœ¨cocottributes-masterä¸Šçš„æˆåŠŸé…ç½®
"""
import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import logging
import argparse
from tqdm import tqdm
import numpy as np
import time
from datetime import datetime
import joblib
from PIL import Image
import torchvision.transforms as transforms
from pycocotools.coco import COCO

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def setup_logging(log_dir):
    """è®¾ç½®æ—¥å¿—"""
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, f'training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    
    return log_file

def get_image_crop(img, x, y, width, height, crop_size=224, padding=16):
    """åŸºäºæˆåŠŸç»éªŒçš„å›¾åƒè£å‰ªå‡½æ•°"""
    scale = crop_size / (crop_size - padding * 2)
    semi_width = width / 2
    semi_height = height / 2
    centerx = x + semi_width
    centery = y + semi_height
    img_width, img_height = img.size
    
    upper = max(0, centery - (semi_height * scale))
    lower = min(img_height, centery + (semi_height * scale))
    left = max(0, centerx - (semi_width * scale))
    right = min(img_width, centerx + (semi_width * scale))
    
    crop_img = img.crop((left, upper, right, lower))
    return crop_img

class COCOAttributesSuccessDataset(Dataset):
    """åŸºäºæˆåŠŸç»éªŒçš„COCO Attributesæ•°æ®é›†"""
    def __init__(self, attributes_file, annotations_file, dataset_root,
                 transforms=None, split='train2014', n_attrs=204):
        self.attributes_dataset = joblib.load(attributes_file)
        self.coco = COCO(annotations_file)
        self.dataset_root = dataset_root
        self.transforms = transforms
        self.split = split
        self.n_attrs = n_attrs
        
        # åŠ è½½æ•°æ®
        self.data = []
        if 'ann_attrs' in self.attributes_dataset:
            logging.info("Using new format attributes dataset with ann_attrs")
            for ann_id, ann_attr in self.attributes_dataset['ann_attrs'].items():
                if ann_attr['split'] == split:
                    self.data.append(ann_id)
        
        logging.info(f"Loaded {len(self.data)} samples for {split}")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        ann_id = self.data[index]
        
        # è·å–å±æ€§å‘é‡
        attrs = self.attributes_dataset['ann_attrs'][ann_id]['attrs_vector']
        ann_id_actual = int(ann_id)
        attrs = (attrs > 0).astype(np.float64)
        
        try:
            ann = self.coco.loadAnns(ann_id_actual)[0]
            image = self.coco.loadImgs(ann['image_id'])[0]
            x, y, width, height = ann["bbox"]
            
            # ä½¿ç”¨2017ç‰ˆæœ¬è·¯å¾„
            split_dir = 'train2017' if self.split == 'train2014' else 'val2017'
            img = Image.open(os.path.join(self.dataset_root, split_dir, image["file_name"])).convert('RGB')
            img = get_image_crop(img, x, y, width, height, 224)
            
            if self.transforms:
                img = self.transforms(img)
            
            # è½¬æ¢ä¸ºé¡¹ç›®éœ€è¦çš„æ ¼å¼
            attrs_tensor = torch.tensor(attrs, dtype=torch.float32)
            targets = {
                'color': attrs_tensor[:12],
                'material': attrs_tensor[12:27],
                'shape': attrs_tensor[27:47],
                'texture': attrs_tensor[47:57],
                'size': attrs_tensor[57:62],
                'other': attrs_tensor[62:70]
            }
            
            return img, targets
            
        except Exception as e:
            # é”™è¯¯å¤„ç†ï¼Œè¿”å›éšæœºæ ·æœ¬
            return self.__getitem__((index + 1) % len(self.data))

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    parser = argparse.ArgumentParser(description='åŸºäºæˆåŠŸç»éªŒçš„COCOAttributesè®­ç»ƒ')
    parser.add_argument('--epochs', type=int, default=40, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--batch_size', type=int, default=16, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--save_dir', type=str, default='./checkpoints_success', help='ä¿å­˜ç›®å½•')
    parser.add_argument('--log_dir', type=str, default='./logs_success', help='æ—¥å¿—ç›®å½•')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    log_file = setup_logging(args.log_dir)
    logging.info("å¼€å§‹åŸºäºæˆåŠŸç»éªŒçš„COCOAttributesè®­ç»ƒ...")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logging.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        # åˆ›å»ºæ•°æ®å˜æ¢
        train_transforms = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        val_transforms = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = COCOAttributesSuccessDataset(
            'D:/KKK/jieoulunwen/data/cocottributes-master/cocottributes-master/MSCOCO/cocottributes_new_version.jbl',
            'D:/KKK/COCO_Dataset/annotations/instances_train2017.json',
            'D:/KKK/COCO_Dataset',
            transforms=train_transforms,
            split='train2014'
        )
        
        val_dataset = COCOAttributesSuccessDataset(
            'D:/KKK/jieoulunwen/data/cocottributes-master/cocottributes-master/MSCOCO/cocottributes_new_version.jbl',
            'D:/KKK/COCO_Dataset/annotations/instances_val2017.json',
            'D:/KKK/COCO_Dataset',
            transforms=val_transforms,
            split='val2014'
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)
        val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)
        
        logging.info(f"è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬, éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
        
        # å¯¼å…¥æ¨¡å‹å’Œé…ç½®
        from config.base_config import get_config
        from models import WeakSupervisedCrossModalAlignment
        from training.losses import ComprehensiveLoss
        
        config = get_config('COCOAttributes')
        config.learning_rate = args.lr
        config.batch_size = args.batch_size
        
        model = WeakSupervisedCrossModalAlignment(config).to(device)
        criterion = ComprehensiveLoss(config)
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(args.save_dir, exist_ok=True)
        
        best_val_loss = float('inf')
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(args.epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            num_batches = 0
            
            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}')
            for images, targets in progress_bar:
                images = images.to(device)
                targets = {k: v.to(device) for k, v in targets.items()}
                
                optimizer.zero_grad()
                outputs = model(images)
                loss, _ = criterion(outputs, targets, epoch)
                
                if not torch.isnan(loss):
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    optimizer.step()
                    train_loss += loss.item()
                    num_batches += 1
                
                avg_loss = train_loss / max(1, num_batches)
                progress_bar.set_postfix({'loss': f'{avg_loss:.4f}'})
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0.0
            val_batches = 0
            
            with torch.no_grad():
                for images, targets in val_loader:
                    images = images.to(device)
                    targets = {k: v.to(device) for k, v in targets.items()}
                    
                    outputs = model(images)
                    loss, _ = criterion(outputs, targets, epoch)
                    
                    if not torch.isnan(loss):
                        val_loss += loss.item()
                        val_batches += 1
            
            scheduler.step()
            
            avg_train_loss = train_loss / max(1, num_batches)
            avg_val_loss = val_loss / max(1, val_batches)
            
            logging.info(f"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'train_loss': avg_train_loss,
                    'val_loss': avg_val_loss,
                }, os.path.join(args.save_dir, 'best_coco_attributes_success.pth'))
                logging.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: {avg_val_loss:.4f}")
        
        logging.info(f"è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        return True
        
    except Exception as e:
        logging.error(f"è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆ!")
    else:
        print("ğŸ’¥ è®­ç»ƒå¤±è´¥!") 